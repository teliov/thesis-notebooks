{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# So we can use the *thesislib* package\n",
    "import sys\n",
    "import os\n",
    "\n",
    "module_path = os.path.abspath(\"..\")\n",
    "\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from dateutil.parser import parse as date_parser\n",
    "from dateutil.relativedelta import relativedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from tabulate import tabulate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from thesislib.utils import pathutils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "patients_csv = pathutils.get_data_file(\"prob-synthea-1/data/patients.csv\")\n",
    "patient_conditions_csv = pathutils.get_data_file(\"prob-synthea-1/data/patient_conditions.csv\")\n",
    "condition_symptom_csv = pathutils.get_data_file(\"prob-synthea-1/data/patient_condition_symptoms.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "patients = pd.read_csv(patients_csv)\n",
    "conditions = pd.read_csv(patient_conditions_csv)\n",
    "symptoms = pd.read_csv(condition_symptom_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "condition_codes = conditions['CODE'].unique().tolist()\n",
    "condition_codes.sort()\n",
    "conditions_db = {code: conditions[conditions['CODE'] == code].iloc[0]['DESCRIPTION'] for code in condition_codes}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "symptom_vector = symptoms['SYMPTOM_CODE'].unique().tolist()\n",
    "symptom_vector.sort()\n",
    "symptoms_db = {code: symptoms[symptoms['SYMPTOM_CODE'] == code].iloc[0]['SYMPTOM_DISPLAY'] for code in symptom_vector}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**\n",
    "\n",
    "If we wanted to grab the full set of symptoms and not just those appearing in the available data:\n",
    "\n",
    "```python\n",
    "with open(\"synthea/symptoms_db.json\") as fp:\n",
    "    symptom_db = json.load(fp)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Urethritis': 37740, 'Asthma': 39700, 'Acute sinusitis': 39458, 'Pharyngitis': 39771, 'Pyelonephritis': 38265, 'Acute bronchitis': 39669, 'Strep throat': 39664, 'Chronic sinusitis': 39675, 'Cystitis': 38200}\n"
     ]
    }
   ],
   "source": [
    "condition_count = {conditions_db[code]: conditions[conditions['CODE'] == code].shape[0] for code in condition_codes}\n",
    "print(condition_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# also encode the conditions one-hot-encoding\n",
    "condition_labels = {code: idx for idx, code in enumerate(condition_codes)}\n",
    "_condition_index_map = {condition_labels[code]: conditions_db[code] for code in condition_labels.keys()}\n",
    "with open(pathutils.get_data_file(\"prob-synthea-1/output/labels_map.json\"), \"w\") as fp:\n",
    "    json.dump(_condition_index_map, fp, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "race_code = {'white': 0, 'black':1, 'asian':2, 'native':3, 'other':4}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine the dataframes\n",
    "# conditions with patients\n",
    "combined = conditions.merge(patients, how='left', left_on='PATIENT', right_on='Id', suffixes=('', '_pat'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# symptoms with conditions\n",
    "complete = symptoms.merge(combined, how='left', left_on='CONDITION_ID', right_on='Id', suffixes=('_symp', ''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_drop = ['DESCRIPTION', 'DIAGNOSED', 'DEATHDATE', 'SSN', 'DRIVERS', 'PASSPORT', 'PREFIX', 'FIRST', 'LAST', \n",
    "              'SUFFIX', 'MAIDEN', 'MARITAL', 'ETHNICITY','BIRTHPLACE', 'ADDRESS', 'CITY', 'STATE',\n",
    "           'COUNTY', 'ZIP', 'LAT', 'LON', 'HEALTHCARE_EXPENSES', 'Id', 'Id_pat', 'SYMPTOM_DISPLAY', 'VALUE_CODE',\n",
    "           'VALUE_DISPLAY', 'HEALTHCARE_COVERAGE'\n",
    "          ]\n",
    "complete = complete.drop(columns=to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# are there conditions\n",
    "# group by the condition id\n",
    "condition_grp = complete.groupby(['CONDITION_ID'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = pathutils.get_data_file(\"prob-synthea-1/output\")\n",
    "if not os.path.isdir(output_dir):\n",
    "    os.mkdir(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dump = os.path.join(output_dir, \"data.json\")\n",
    "if os.path.exists(data_dump):\n",
    "    with open(data_dump) as fp:\n",
    "        design_matrix = json.load(fp)\n",
    "else:\n",
    "    design_matrix = {\n",
    "        \"label\": [],\n",
    "        \"age\": [],\n",
    "        \"gender\": [],\n",
    "        \"race\": [],\n",
    "    }\n",
    "\n",
    "    for item in symptom_vector:\n",
    "        design_matrix[item] = []\n",
    "    # build the design matrix\n",
    "    for item, df in condition_output_diriter__():\n",
    "        vector = {_: 0 for _ in symptom_vector}\n",
    "\n",
    "        onset_date = date_parser(df['ONSET'].iloc[0])\n",
    "        patient_birthdate = date_parser(df[\"BIRTHDATE\"].iloc[0])\n",
    "        vector['age'] =  abs(patient_birthdate.year - onset_date.year)\n",
    "        vector['gender'] = 0 if df['GENDER'].iloc[0] == 'F' else 1\n",
    "        vector['race'] = race_code[df['RACE'].iloc[0]]\n",
    "        vector['label'] = condition_labels[df['CODE'].iloc[0]]\n",
    "\n",
    "        # fill in the observations\n",
    "        for idx, symptom_code in df[\"SYMPTOM_CODE\"].items():\n",
    "            vector[symptom_code] = 1\n",
    "\n",
    "        for k,v in vector.items():\n",
    "            design_matrix[k].append(v)\n",
    "    with open(data_dump, 'w') as fp:\n",
    "        json.dump(design_matrix, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dump = os.path.join(output_dir, \"train.json\")\n",
    "test_dump = os.path.join(output_dir, \"test.json\")\n",
    "\n",
    "if os.path.exists(test_dump) and os.path.exists(train_dump):\n",
    "    train_data = pd.read_json(train_dump)\n",
    "    test_data = pd.read_json(test_dump)\n",
    "    \n",
    "    train_labels = train_data['labels']\n",
    "    train_df = train_data.drop(columns=[\"labels\"])\n",
    "    test_labels = test_data['labels']\n",
    "    test_df = test_data.drop(columns=[\"labels\"])\n",
    "else:\n",
    "    data_df = pd.DataFrame(design_matrix)\n",
    "    \n",
    "    # let's keep a test set which we would use for evaluation, \n",
    "    split_t = StratifiedShuffleSplit(n_splits=1, test_size=0.1, random_state=0)\n",
    "    data_X = data_df.drop(columns=['label'])\n",
    "    data_Y = data_df['label']\n",
    "    \n",
    "    train_df = None\n",
    "    train_labels = None\n",
    "    test_df = None\n",
    "    test_labels = None\n",
    "    \n",
    "    for train_index, test_index in split_t.split(data_X, data_Y):\n",
    "        train_df = data_X.iloc[train_index]\n",
    "        test_df = data_X.iloc[test_index]\n",
    "        train_labels = data_Y.iloc[train_index]\n",
    "        test_labels = data_Y.iloc[test_index]\n",
    "    \n",
    "    train_data = train_df.copy()\n",
    "    train_data['labels'] = train_labels\n",
    "    \n",
    "    test_data = test_df.copy()\n",
    "    test_data['labels'] = test_labels\n",
    "    \n",
    "    train_data.to_json(train_dump)\n",
    "    test_data.to_json(test_dump)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = RandomForestClassifier(n_estimators=140, criterion='gini', max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features='auto', max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, bootstrap=True, oob_score=False, n_jobs=2, random_state=None, verbose=0, warm_start=False, class_weight=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = clf.fit(train_df, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictions on the train set\n",
    "train_predictions = res.predict(train_df)\n",
    "diff = (train_predictions - train_labels) != 0\n",
    "num_missed = np.sum(diff)\n",
    "num_labels = len(train_predictions)\n",
    "accuracy = (num_labels - num_missed)*1.0/num_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set: Missed 23585 predictions out of 316650 samples for an accuracy of 0.926\n"
     ]
    }
   ],
   "source": [
    "print(\"Train set: Missed %d predictions out of %d samples for an accuracy of %.3f\" % (num_missed, num_labels, accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions = res.predict(test_df)\n",
    "diff = (test_predictions - test_labels) != 0\n",
    "num_missed = np.sum(diff)\n",
    "num_labels = len(test_predictions)\n",
    "accuracy = (num_labels - num_missed) * 1.0/num_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set: Missed 4811 predictions out of 35184 samples for an accuracy of 0.863\n"
     ]
    }
   ],
   "source": [
    "print(\"Test set: Missed %d predictions out of %d samples for an accuracy of %.3f\" % (num_missed, num_labels, accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnf_mat = confusion_matrix(test_labels, test_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3584, 0, 0, 0, 3, 0, 0, 0, 179, 11]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnf_mat[0, :].tolist() + [11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Ur    As    AcSi    Ph    Py    AcBr    StTh    ChSi    Cy\n",
      "----  ----  ----  ------  ----  ----  ------  ------  ------  ----\n",
      "Ur    3584     0       0     0     3       0       0       0   179\n",
      "As       0  3607       3     5     0     327       1      11     0\n",
      "AcSi     0    16    2400     8     0      63      77    1382     0\n",
      "Ph       0    21      17  3857     0      21      53       8     0\n",
      "Py       2     1       0     1  3747       0       0       0    75\n",
      "AcBr     0   515      44    18     0    3252      48      90     0\n",
      "StTh     0     7      18    17     0       6    3840      78     0\n",
      "ChSi     0    48    1230    13     0      82     154    2439     0\n",
      "Cy     113     0       0     0    56       0       0       0  3647\n"
     ]
    }
   ],
   "source": [
    "condition_names = []\n",
    "for val in condition_codes:\n",
    "    parts = conditions_db[val].split(\" \")\n",
    "    if len(parts) == 1:\n",
    "        itm = parts[0]\n",
    "        condition_names.append(itm[0].upper() + itm[1].lower())\n",
    "    else:\n",
    "        itm = \"\"\n",
    "        for st in parts:\n",
    "            itm += st[0].upper() + st[1].lower()\n",
    "        condition_names.append(itm)\n",
    "\n",
    "\n",
    "table = []\n",
    "for idx in range(len(condition_names)):\n",
    "    data = [condition_names[idx]] + cnf_mat[idx, :].tolist()\n",
    "    table.append(data)\n",
    "\n",
    "print(tabulate(table, headers=condition_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export Symptoms to txt. \n",
    "###### To run this change the jupyter cell type to \"code\" from \"markdown\"\n",
    "\n",
    "```python\n",
    "symptoms_file = pathutils.get_data_file(\"prob-synthea-1/data/symptoms_db.json\")\n",
    "with open(symptoms_file) as fp:\n",
    "    d = json.load(fp)\n",
    "values = list(d.values())\n",
    "values.sort()\n",
    "\n",
    "symptoms_txt = pathutils.get_data_file(\"prob-synthea-1/data/symptoms_list.txt\")\n",
    "with open(symptoms_txt, \"w\") as fp:\n",
    "    fp.write(\"SLUG, DESCRIPTION\\n\")\n",
    "    for val in values:\n",
    "        parts = \" \".join(val.split(\"-\"))\n",
    "        fp.write(\"%s, %s\\n\" % (val, parts))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
