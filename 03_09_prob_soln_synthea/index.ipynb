{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# So we can use the *thesislib* package\n",
    "import sys\n",
    "import os\n",
    "\n",
    "module_path = os.path.abspath(\"..\")\n",
    "\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib import reload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from thesislib.utils import pathutils\n",
    "from thesislib.utils.ml import report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dump = pathutils.get_data_file(\"prob-synthea-1/output/train.json\")\n",
    "test_dump = pathutils.get_data_file(\"prob-synthea-1/output/test.json\")\n",
    "\n",
    "train_df = pd.read_json(train_dump)\n",
    "test_df = pd.read_json(test_dump)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Idea\n",
    "- Assume naive bayes\n",
    "- so :\n",
    "\n",
    "    $\n",
    "    Pr(condition|age, gender, race, {symptoms}) = Pr(condition|age) * Pr(condition|gender) * Pr(condition|race) * Pr(condition|{symptoms})\n",
    "    $\n",
    "- For the symptoms, we can assume the beta distribution\n",
    "- Now, because we have quite a bit of data, the simplest solution would be to assume that the beta distribution is in fact your regular bernoulli\n",
    "- If we stick strictly to the Beta distribution, we would need to evaluate the hyper parameters a and b, a grid search would be good for this\n",
    "- An experiment should be carried out to estimate values of a and b for different sizes of training data\n",
    "- But we know that when the dataset becomes large enough, then the effects of a and be become much reduced.\n",
    "- Underlying assumption here is that we have a \"large enough\" dataset\n",
    "\n",
    "For each available condition, we need to evaluate the $\\mu$ values that fits the probability distribution for each symptom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "symptoms = [itm for itm in train_df.columns if len(itm) == 56]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "conditions = train_df['labels'].unique().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "condition_symptom_pr_map = {cnd: {symp: 0 for symp in symptoms} for cnd in conditions}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill up the pr map:\n",
    "grp = train_df.groupby(['labels'])\n",
    "for itm, _df in grp.__iter__():\n",
    "    _probs = _df[symptoms].sum()/_df.shape[0]\n",
    "    condition_symptom_pr_map[itm].update(_probs.to_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# update, added to handle the case of 0 probabilities\n",
    "eps = 0.001\n",
    "for cnd in condition_symptom_pr_map:\n",
    "    for symp in condition_symptom_pr_map[cnd]:\n",
    "        val = condition_symptom_pr_map[cnd][symp]\n",
    "        if val < eps:\n",
    "            val = eps\n",
    "        elif val > (1 - eps):\n",
    "            print(\"hehere\", val)\n",
    "            val = 1 - eps\n",
    "        condition_symptom_pr_map[cnd][symp] = val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "pr_map = condition_symptom_pr_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's do a naive bayes implementation\n",
    "# note that so far we have ignored the predictive cababilities of the age, race and gender\n",
    "# ideally we should also attempt to fit some distribution to these two\n",
    "# a multinomial distribution for race, a gaussian for age and also a binomial for gender\n",
    "def naive_bayes_evaluation(test_sample, pr_map, symptoms):\n",
    "    _df = test_sample[symptoms].to_dict()\n",
    "    cnd_prob = {cnd: 0 for cnd in pr_map}\n",
    "    predicted = None\n",
    "    curr_prob = None\n",
    "    for cnd in pr_map:\n",
    "        prob = 1.0\n",
    "        for sym, val in pr_map[cnd].items():\n",
    "            if _df[sym] == 1:\n",
    "                prob *= val\n",
    "            else:\n",
    "                prob *= (1-val)\n",
    "        if predicted is None:\n",
    "            predicted = cnd\n",
    "            curr_prob = prob\n",
    "        elif prob > curr_prob:\n",
    "            curr_prob = prob\n",
    "            predicted = cnd\n",
    "        cnd_prob[cnd] = prob\n",
    "    \n",
    "    return predicted, cnd_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions = [naive_bayes_evaluation(_itm, condition_symptom_pr_map, symptoms)[0] for idx, _itm in test_df.iterrows()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_labels = test_df['labels']\n",
    "diff = (test_predictions - test_labels) != 0\n",
    "num_missed = np.sum(diff)\n",
    "num_labels = len(test_predictions)\n",
    "accuracy = (num_labels - num_missed) * 1.0/num_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set: Missed 4283 predictions out of 35184 samples for an accuracy of 0.878\n"
     ]
    }
   ],
   "source": [
    "print(\"Test set: Missed %d predictions out of %d samples for an accuracy of %.3f\" % (num_missed, num_labels, accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This result is better than the results by the RandomForest (86.3%) on the same dataset.\n",
    "\n",
    "Of course this also fits the data even more perfectly, so it's ability to generalize would be very low. \n",
    "\n",
    "One way to attempt to aleviate this potential problem would be to stick with the assumption of the beta distribution for the symptoms. But again, there is still the likelihood of overfitting to the data when we perform a grid search for optimal hyper parameters for the beta distribution.\n",
    "\n",
    "One other problem with this approach is the fact that $Pr(condition|symptom_i)$ can be 0, a more sensible approach would be to replace all 0's with a very small probability say $0.001$\n",
    "\n",
    "Also need a much faster implementation for the naive bayes implementation!\n",
    "\n",
    "Thinking now about the way the data was modelled. The most indicative symptoms for a condition usually occur the least but once they occur it drastically increases the probability that a certain condition is responsible for the other symptoms.\n",
    "To the best of my knowledge synthea's modelling processes does not capture this thinking, might be good to see/ask how best to make this modelling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(pathutils.get_data_file(\"prob-synthea-1/output/labels_short_map.json\")) as fp:\n",
    "    label_map = json.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The confusion matrix\n",
    "_, cnf_mat_str = report.pretty_print_confusion_matrix(test_labels, test_predictions, label_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        U    Ast    AS    Ph    Py    AB    ST    CS    Cy\n",
      "---  ----  -----  ----  ----  ----  ----  ----  ----  ----\n",
      "U    3557      0     0     0     0     0     0     0   209\n",
      "Ast     0   3722     0     0     0   216     0    16     0\n",
      "AS      0     13  2490     1     0    58    89  1295     0\n",
      "Ph      0     24     6  3856     0    18    60    13     0\n",
      "Py      1      1     0     0  3742     0     0     0    82\n",
      "AB      0    474    20    14     0  3312    53    94     0\n",
      "ST      0      8     0     1     0     0  3903    54     0\n",
      "CS      0     55  1041     3     0    71   185  2611     0\n",
      "Cy     86      0     0     0    22     0     0     0  3708\n"
     ]
    }
   ],
   "source": [
    "print(cnf_mat_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# can we make a vectorized format of the naive bayes ?\n",
    "def naive_bayes_evaluation_vec(test_df, pr_map, symptoms):\n",
    "    _df = test_df[symptoms]\n",
    "    \n",
    "    def _xform(item, pr):\n",
    "        prob = 1.0\n",
    "        for idx in item.keys():\n",
    "            val = item.get(idx)\n",
    "            item.loc[idx] = (pr[idx]*val + (1-pr[idx])*(1-val))\n",
    "        return item\n",
    "    \n",
    "    for cnd in pr_map:\n",
    "        pr = pr_map[cnd]\n",
    "        _df_trans = _df.transform(_xform, pr)\n",
    "    \n",
    "    cnd_prob = {cnd: 0 for cnd in pr_map}\n",
    "    predicted = None\n",
    "    curr_prob = None\n",
    "    for cnd in pr_map:\n",
    "        prob = 1.0\n",
    "        for sym, val in pr_map[cnd].items():\n",
    "            if _df[sym] == 1:\n",
    "                prob *= val\n",
    "            else:\n",
    "                prob *= (1-val)\n",
    "        if predicted is None:\n",
    "            predicted = cnd\n",
    "            curr_prob = prob\n",
    "        elif prob > curr_prob:\n",
    "            curr_prob = prob\n",
    "            predicted = cnd\n",
    "        cnd_prob[cnd] = prob\n",
    "    \n",
    "    return predicted, cnd_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _xform(item, pr_map):\n",
    "    _tf = pd.DataFrame(data={item: 1 for item in pr_map.keys()}, index=[0])\n",
    "    \n",
    "    for cnd in pr_map:\n",
    "        sym_pr = pr_map[cnd]\n",
    "        prob = 1.0\n",
    "        for idx in item.keys():\n",
    "            val = item.get(idx)\n",
    "            prob *= (sym_pr[idx]*val + (1-sym_pr[idx])*(1-val))\n",
    "        _tf[cnd] = prob\n",
    "    return _tf\n",
    "\n",
    "_ = td.head(1).transform(_xform, axis=1, pr_map=pr_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _xform_1(item):\n",
    "    _tf = pd.DataFrame(data=[[1 for item in range(len(pr_map))]])\n",
    "    return _tf\n",
    "\n",
    "_ = td.head(1).apply(_xform_1, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's try a different approach ehn\n",
    "# sklearn has naive bayes (NB) classifiers for Bernoulli like variables, and for gaussian.\n",
    "# since NB is multiplicative, we should be able to multiply and arrive at the same results!\n",
    "from sklearn import naive_bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = train_df['labels']\n",
    "_df = train_df[symptoms]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BernoulliNB(alpha=1.0, binarize=0.0, class_prior=None, fit_prior=False)"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the naive bayes classifier\n",
    "nb_clf = naive_bayes.BernoulliNB(fit_prior=False)\n",
    "nb_clf.fit(_df, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict on the test test\n",
    "_tf = test_df[symptoms]\n",
    "test_labels = test_df['labels']\n",
    "\n",
    "test_predictions_1 = nb_clf.predict(_tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set: Missed 4283 predictions out of 35184 samples for an accuracy of 0.878\n"
     ]
    }
   ],
   "source": [
    "diff = (test_predictions_1 - test_labels) != 0\n",
    "num_missed = np.sum(diff)\n",
    "num_labels = len(test_predictions_1)\n",
    "accuracy = (num_labels - num_missed) * 1.0/num_labels\n",
    "print(\"Test set: Missed %d predictions out of %d samples for an accuracy of %.3f\" % (num_missed, num_labels, accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        U    Ast    AS    Ph    Py    AB    ST    CS    Cy\n",
      "---  ----  -----  ----  ----  ----  ----  ----  ----  ----\n",
      "U    3557      0     0     0     0     0     0     0   209\n",
      "Ast     0   3722     0     0     0   216     0    16     0\n",
      "AS      0     13  2490     1     0    58    89  1295     0\n",
      "Ph      0     24     6  3856     0    18    60    13     0\n",
      "Py      1      1     0     0  3742     0     0     0    82\n",
      "AB      0    474    20    14     0  3312    53    94     0\n",
      "ST      0      8     0     1     0     0  3903    54     0\n",
      "CS      0     55  1041     3     0    71   185  2611     0\n",
      "Cy     86      0     0     0    22     0     0     0  3708\n"
     ]
    }
   ],
   "source": [
    "_, cnf_mat_str = report.pretty_print_confusion_matrix(test_labels, test_predictions_1, label_map)\n",
    "print(cnf_mat_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the same result as our initial implementation, only faster, way faster!\n",
    "\n",
    "I believe we can combine different NaiveBayes to make use of all the available data. Would need to study the sklearn implementation of the naive bayes to understand how best to combine them all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.utils.fixes import logsumexp\n",
    "\n",
    "class ThesisNaiveBayes:\n",
    "    def __init__(self, classifier_map):\n",
    "        self.classifier_map = classifier_map\n",
    "        self._dfs = [None for idx in range(len(classifier_map))]\n",
    "        self.fitted = False\n",
    "        \n",
    "    def _joint_log_likelihood(self, X):\n",
    "        if not self.fitted:\n",
    "            raise ValueError(\"Model has not been fitted.\")\n",
    "        \n",
    "        _probs = np.zeros((X.shape[0], self.classes.shape[0]))\n",
    "        for idx in range(len(self.classifier_map)):\n",
    "            clf, keys = self.classifier_map[idx]\n",
    "            _df = X[keys]\n",
    "            _probs += clf._joint_log_likelihood(_df)\n",
    "        \n",
    "        return _probs\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        labelbin = LabelBinarizer()\n",
    "        _y = labelbin.fit_transform(y)\n",
    "        self.classes = labelbin.classes_\n",
    "        for idx in range(len(self.classifier_map)):\n",
    "            clf, keys = self.classifier_map[idx]\n",
    "            _df = X[keys]\n",
    "            clf.fit(_df, y)\n",
    "            self.classifier_map[idx][0] =  clf\n",
    "        \n",
    "        self.fitted = True\n",
    "        \n",
    "    def predict_log_proba(self, X):\n",
    "        if not self.fitted:\n",
    "            raise ValueError(\"Model has not been fitted.\")\n",
    "        \n",
    "        _probs = self._joint_log_likelihood(X)\n",
    "        # normalize (see naive_bayes.py in sklearn for explanation!!)\n",
    "        _log_prob_x = logsumexp(_probs, axis=1)\n",
    "        return _probs - np.atleast_2d(_log_prob_x).T\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        return np.exp(self.predict_log_proba(X))\n",
    "    \n",
    "    def predict(self, X):\n",
    "        if not self.fitted:\n",
    "            raise ValueError(\"Model has not been fitted.\")\n",
    "        jll = self._joint_log_likelihood(X)\n",
    "        return self.classes[np.argmax(jll, axis=1)]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from thesislib.utils.ml import models\n",
    "from sklearn import naive_bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "_df = train_df.drop(columns=['labels'])\n",
    "_tf = test_df.drop(columns=['labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'thesislib.utils.ml.models' from '/Users/teliov/TUD/Thesis/Medvice/Notebooks/thesislib/utils/ml/models.py'>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reload(models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = len(conditions)\n",
    "equal_class_prior = [1.0/num_classes for idx in range(num_classes)]\n",
    "\n",
    "symptom_gender_clf = naive_bayes.BernoulliNB(fit_prior=False)\n",
    "race_clf = naive_bayes.MultinomialNB(fit_prior=False)\n",
    "age_clf = naive_bayes.GaussianNB(priors=equal_class_prior)\n",
    "\n",
    "symtom_gender = [\"gender\"] + symptoms\n",
    "\n",
    "classifier_map = [[symptom_gender_clf, symtom_gender], [race_clf, [\"race\"]], [age_clf, [\"age\"]]]\n",
    "tnb = models.ThesisNaiveBayes(classifier_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "tnb.fit(_df, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set: Missed 4233 predictions out of 35184 samples for an accuracy of 0.880\n"
     ]
    }
   ],
   "source": [
    "diff = (test_predictions_2 - test_labels) != 0\n",
    "num_missed = np.sum(diff)\n",
    "num_labels = len(test_predictions_2)\n",
    "accuracy = (num_labels - num_missed) * 1.0/num_labels\n",
    "print(\"Test set: Missed %d predictions out of %d samples for an accuracy of %.3f\" % (num_missed, num_labels, accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get a slightly better performance when we include the race, gender and age as part of the predictive properties.\n",
    "\n",
    "The simplicity of the NaiveBayes allows for a straightforward combination of different individual classifiers.\n",
    "\n",
    "However, been reading up and while NaiveBayes is a pretty decent classifier especially when you have quite a bit of data, it is not a good estimator. This means that though it would most likely get the correct class more often than not, you can't place much trust in the value of the probabilities that it outputs as a measure of ranking the likelihood of other classes. \n",
    "\n",
    "This would not help in our goal of differential diagnosis, we need reliable estimates.\n",
    "\n",
    "This is something that we would need to verify however!\n",
    "\n",
    "But in the meantime, we now have an efficient way for vectorizing the computation for Naive Bayes that would definitely scale with the size of the dataset.\n",
    "\n",
    "Would also be a good idea to study in-depth the implementation of the naive bayes solution in sklearn to pickup some good vectorization skills, the speed up compared to the linear approach I had before is astronomical!!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
