{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# So we can use the *thesislib* package\n",
    "import sys\n",
    "import os\n",
    "\n",
    "module_path = os.path.abspath(\"..\")\n",
    "\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from thesislib.utils import pathutils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from dateutil.parser import parse as date_parser\n",
    "from dateutil.relativedelta import relativedelta\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "conditions_csv = pathutils.get_data_file(\"plain-synthea/data/conditions.csv\")\n",
    "conditions = pd.read_csv(conditions_csv)\n",
    "\n",
    "encounters_csv = pathutils.get_data_file(\"plain-synthea/data/encounters.csv\")\n",
    "encounters = pd.read_csv(encounters_csv)\n",
    "\n",
    "observations_csv = pathutils.get_data_file(\"plain-synthea/data/observations.csv\")\n",
    "observations = pd.read_csv(observations_csv)\n",
    "\n",
    "patients_csv = pathutils.get_data_file(\"plain-synthea/data/patients.csv\")\n",
    "patients = pd.read_csv(patients_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "conditions_db_file = pathutils.get_data_file(\"plain-synthea/data/conditions.json\")\n",
    "with open(conditions_db_file) as f:\n",
    "    conditions_db = json.load(f)\n",
    "\n",
    "observations_db_file = pathutils.get_data_file(\"plain-synthea/data/observations.json\")\n",
    "with open(observations_db_file) as f:\n",
    "    observations_db = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some important parameters\n",
    "\n",
    "# these are so called \"encounters for symptoms\", \n",
    "# The rational is that they usually indicate when the conditions are diagnosed and as such might \n",
    "# provide the right kind of information!\n",
    "SYMPTOM_ENCOUNTER_CODE = 185345009"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utils\n",
    "def get_output_folder():\n",
    "    \"\"\"\n",
    "    Returns the output folder where generated data should be stored\n",
    "    \"\"\"\n",
    "    output_folder = os.path.join(pathutils.get_data_directory(), \"plain-synthea/output\")\n",
    "    \n",
    "    if not os.path.isdir(output_folder):\n",
    "        os.mkdir(output_folder)\n",
    "\n",
    "    return output_folder\n",
    "\n",
    "def _count_key(label_count):\n",
    "    return label_count[2]\n",
    "\n",
    "def _print_list(obj):\n",
    "    \"\"\"\n",
    "    Pretty list printing\n",
    "    \"\"\"\n",
    "    for item in obj:\n",
    "        print(item)\n",
    "\n",
    "def form_design_matrix(observation_db):\n",
    "    \"\"\"\n",
    "    Using the observations, builds a 'design matrix' that would hold all the observed\n",
    "    features!\n",
    "    \"\"\"\n",
    "    if type(observation_db) == \"str\":\n",
    "        with open(observation_db) as fp:\n",
    "            res = json.load(fp)\n",
    "        observation_db = res\n",
    "    data_matrix = {\n",
    "        \"condition_code\": [],\n",
    "        \"condition_start\": [],\n",
    "        \"condition_stop\": [],\n",
    "        \"patient_id\": [],\n",
    "        \"encounter_id\": [],\n",
    "        \"patient_age\": [],\n",
    "        \"marital_status\": [],\n",
    "        \"race\": [],\n",
    "        \"ethnicity\": [],\n",
    "        \"gender\": [],\n",
    "    }\n",
    "\n",
    "    data_matrix.update({k: [] for k in observations_db.keys()})\n",
    "    return data_matrix\n",
    "\n",
    "def get_design_matrix(observation_db, combined_df):\n",
    "    \"\"\"\n",
    "    using a dataframe that combines the selected conditions, encounters and patients,\n",
    "    this function fills up the design matrix with the proper values!\n",
    "    \"\"\"\n",
    "    data_matrix = form_design_matrix(observation_db)\n",
    "    \n",
    "    grouped = reduced_combined.groupby([\"ENCOUNTER\", \"CODE\"])\n",
    "    \n",
    "    data_keys = list(data_matrix.keys())\n",
    "    for item, df in grouped.__iter__():\n",
    "        vector = {k: np.nan for k in data_keys}\n",
    "        vector[\"encounter_id\"] = item[0]\n",
    "        vector[\"condition_code\"] = item[1]\n",
    "        vector[\"condition_start\"] = df[\"START\"].iloc[0]\n",
    "        vector[\"condition_stop\"] = df[\"STOP\"].iloc[0]\n",
    "        vector[\"patient_id\"] = df[\"PATIENT\"].iloc[0]\n",
    "        vector[\"marital_status\"] = df[\"MARITAL\"].iloc[0]\n",
    "        vector[\"race\"] = df[\"RACE\"].iloc[0]\n",
    "        vector[\"ethnicity\"] = df[\"ETHNICITY\"].iloc[0]\n",
    "        vector[\"gender\"] = df[\"GENDER\"].iloc[0]\n",
    "\n",
    "        # fill in the observations\n",
    "        for idx, obv_code in df[\"CODE_obv\"].items():\n",
    "            if obv_code not in data_keys:\n",
    "                continue\n",
    "            vector[obv_code] = df[\"VALUE\"].loc[idx]\n",
    "\n",
    "        # handle the age\n",
    "        start_encounter_date = date_parser(df[\"START_enc\"].iloc[0])\n",
    "        patient_birthdate = date_parser(df[\"BIRTHDATE\"].iloc[0])\n",
    "        vector[\"patient_age\"] = abs(patient_birthdate.year - start_encounter_date.year)\n",
    "\n",
    "        for k,v in vector.items():\n",
    "            data_matrix[k].append(v)\n",
    "    return data_matrix\n",
    "\n",
    "def filter_data(design_db):\n",
    "    \"\"\"\n",
    "    Using a design dataframe, this function drops columns that have no data\n",
    "    \"\"\"\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = get_output_folder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "symptom_encounters = encounters.loc[encounters[\"CODE\"] == SYMPTOM_ENCOUNTER_CODE]\n",
    "# which conditions are related to these encounters\n",
    "symptom_conditions = conditions.loc[conditions['ENCOUNTER'].isin(symptom_encounters[\"Id\"])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# group the conditions by code\n",
    "conditions_group = symptom_conditions.groupby([\"CODE\"])\n",
    "condition_label_counts = []\n",
    "for group_name, group in conditions_group.__iter__():\n",
    "    condition_label_counts.append((conditions_db.get(str(group_name)), group_name, len(group)))\n",
    "\n",
    "condition_label_counts = sorted(condition_label_counts, key=_count_key, reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[444814009, 195662009, 10509002, 40055000, 65363002, 43878008, 75498004, 36971009, 301011002, 232353008]\n"
     ]
    }
   ],
   "source": [
    "top_10_conditions = [item[1] for item in condition_label_counts[:10]]\n",
    "print(top_10_conditions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# based on the condition label counts we take the top 10 conditions\n",
    "# target_condition_codes = [444814009, 195662009, 10509002, 40055000, 65363002, 43878008, 75498004, 36971009, 301011002, 232353008]\n",
    "target_condition_codes = top_10_conditions\n",
    "\n",
    "condition_labels = {code: idx for idx, code in enumerate(target_condition_codes)}\n",
    "# save the labels using a 1 of K encoding scheme\n",
    "condition_labels_json_file = os.path.join(DATA_DIR, \"condition_labels.json\")\n",
    "with open(condition_labels_json_file, 'w') as f:\n",
    "    json.dump(condition_labels, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_conditions = symptom_conditions.loc[symptom_conditions[\"CODE\"].isin(target_condition_codes)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge conditions, encounters and patients so we have this huge design matrix\n",
    "combined = target_conditions.merge(observations, how='left', left_on='ENCOUNTER', right_on='ENCOUNTER', suffixes=('', '_obv'))\n",
    "# merge with patients\n",
    "combined = combined.merge(patients, how='left', left_on='PATIENT', right_on='Id', suffixes=('', '_pat'))\n",
    "# merge with encounters\n",
    "combined = combined.merge(encounters, how='left', left_on='ENCOUNTER', right_on='Id', suffixes=('', '_enc'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# columns that need to be droped. The they don't hold any relevant information ..\n",
    "# how do we know they don't hold any useful information ?? Erm well ..\n",
    "to_drop = ['ADDRESS', 'SSN', 'DRIVERS', 'PASSPORT', 'PREFIX', 'FIRST', 'LAST', 'SUFFIX', 'MAIDEN',\n",
    "           'BIRTHPLACE', 'ADDRESS', 'CITY', 'STATE', 'COUNTY', 'ZIP','LAT', 'LON', 'HEALTHCARE_EXPENSES', 'HEALTHCARE_COVERAGE',\n",
    "           'PROVIDER', 'PAYER', 'BASE_ENCOUNTER_COST', 'TOTAL_CLAIM_COST', 'PAYER_COVERAGE'\n",
    "          ]\n",
    "reduced_combined = combined.drop(columns=to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_matrix = get_design_matrix(observations_db_file, reduced_combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# form the dataframe and save to our results folder\n",
    "data_df = pd.DataFrame(data_matrix)\n",
    "data_df.to_csv(path_or_buf=os.path.join(DATA_DIR, \"data.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The most available observation: Oral temperature with 8720 occurences\n",
      "The least available observation: Polyp size greatest dimension by CAP cancer protocols with 0 occurences\n"
     ]
    }
   ],
   "source": [
    "# which observations have the most available data. The dataset is super sparse, but by how much?\n",
    "observation_keys = list(observations_db.keys())\n",
    "available_count = [data_df[k].notnull().sum() for k in observation_keys]\n",
    "\n",
    "aidx = np.argmax(available_count)\n",
    "uidx = np.argmin(available_count)\n",
    "print(\"The most available observation: %s with %d occurences\" % (observations_db[observation_keys[aidx]], available_count[aidx]))\n",
    "print(\"The least available observation: %s with %d occurences\" % (observations_db[observation_keys[uidx]], available_count[uidx]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# which observations are completely not available\n",
    "completely_empty = np.where(np.array(available_count) < 20)[0]\n",
    "completely_empty_codes = [observation_keys[idx] for idx in completely_empty]\n",
    "completely_empty_dicts = {k: observations_db[k] for k in completely_empty_codes}\n",
    "\n",
    "completely_empty_json_file = os.path.join(DATA_DIR, \"completely_empty.json\")\n",
    "# put this in a json\n",
    "with open(completely_empty_json_file, \"w\") as f:\n",
    "    json.dump(completely_empty_dicts, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop columns that have no data in them!\n",
    "filtered_data = data_df.drop(columns=completely_empty_codes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save to csv\n",
    "filtered_data_csv_file = os.path.join(DATA_DIR, \"filtered_data.csv\")\n",
    "filtered_data.to_csv(path_or_buf=filtered_data_csv_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do some more cleaning to get things more ready to be fed into some training alg.\n",
    "# some useful functions\n",
    "def _condition_transform_fxn(value, labels={}):\n",
    "    return labels[value]\n",
    "\n",
    "def _gender_transform_fxn(value):\n",
    "    if value == 'M':\n",
    "        return 1 # encoding for Male\n",
    "    elif value == 'F':\n",
    "        return 0 # encoding for female\n",
    "    else:\n",
    "        return 2 # encode the nan's\n",
    "\n",
    "def _marital_transform_fxn(value):\n",
    "    if value == 'M':\n",
    "        return 1 # encoding for Married\n",
    "    elif value == 'S':\n",
    "        return 0 # encoding for single\n",
    "    else:\n",
    "        return 2 # encode the nan's\n",
    "\n",
    "def _race_transform_fxn(value):\n",
    "    if value == 'white':\n",
    "        return 0\n",
    "    elif value == 'black':\n",
    "        return 1\n",
    "    elif value == 'asian':\n",
    "        return 2\n",
    "    elif value == 'native':\n",
    "        return 3\n",
    "    elif value == 'other':\n",
    "        return 4\n",
    "    else:\n",
    "        return value # nan's ?? there didn;t seem to be any though"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_data['condition_labels'] = filtered_data['condition_code'].transform(_condition_transform_fxn, labels=condition_labels)\n",
    "filtered_data['marital_status_code'] = filtered_data['marital_status'].transform(_marital_transform_fxn)\n",
    "filtered_data['gender_code'] = filtered_data['gender'].transform(_gender_transform_fxn)\n",
    "filtered_data['race_code'] = filtered_data['race'].transform(_race_transform_fxn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we also need to handle the categorical observation data\n",
    "# there are two of them\n",
    "nominal_observations = ['72166-2'] # tobacco smoking status\n",
    "import imput\n",
    "\n",
    "def _transform_obv(value, code=None):\n",
    "    return imput.get_encoding(code, value, imput.NA_GUESS)\n",
    "\n",
    "for obv in nominal_observations:\n",
    "    new_obv = obv + \"_code\"\n",
    "    filtered_data[new_obv] = filtered_data[obv].transform(_transform_obv, code=obv)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_data = filtered_data.drop(columns=['encounter_id', 'condition_code', 'condition_start', 'condition_stop'])\n",
    "\n",
    "# save this as well\n",
    "ml_data_csv_file = os.path.join(DATA_DIR, \"filtered_for_ml.csv\")\n",
    "ml_data.to_csv(ml_data_csv_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prep a test set that we won't ever touch, so we can do some unbiased evaluation. \n",
    "train_split = 0.8\n",
    "test_split = 0.2\n",
    "\n",
    "train_df = pd.DataFrame(data=None, columns=ml_data.columns)\n",
    "test_df = pd.DataFrame(data=None, columns=ml_data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_grp = ml_data.groupby(['condition_labels'])\n",
    "for label, df in label_grp.__iter__():\n",
    "    index = df.index\n",
    "    num_train = math.ceil(train_split * len(index))\n",
    "    train_selection = np.random.choice(index, num_train, replace=False)\n",
    "\n",
    "    # add these to the train set\n",
    "    train_df = train_df.append(df.loc[train_selection])\n",
    "    \n",
    "    # add what's left\n",
    "    test_selection = list (set(index) - set(train_selection))\n",
    "    test_df = test_df.append(df.loc[test_selection])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the dataframes to csv\n",
    "train_csv_file = os.path.join(DATA_DIR, \"train.csv\")\n",
    "test_csv_file = os.path.join(DATA_DIR, \"test.csv\")\n",
    "\n",
    "train_df.to_csv(train_csv_file)\n",
    "test_df.to_csv(test_csv_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
