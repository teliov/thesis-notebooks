{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Solving the Taxi RL Problem Using DQN\n",
    "\n",
    "Heavily inspired (with slight modifications) from https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html\n",
    "\"\"\"\n",
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple, OrderedDict, Sequence\n",
    "from itertools import count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "Transition = namedtuple('Transition', ('state', 'action', 'next_state', 'reward'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayMemory:\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.memory = []\n",
    "        self.position = 0\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"Saves a transition.\"\"\"\n",
    "        if len(self.memory) < self.capacity:\n",
    "            self.memory.append(None)\n",
    "        self.memory[self.position] = Transition(*args)\n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNUnFancy(nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(DQN, self).__init__()\n",
    "        self._layer1 = nn.Linear(input_dim, 48)\n",
    "        self._layer2 = nn.Linear(48, 32)\n",
    "        self._layer3 = nn.Linear(32, 32)\n",
    "        self._output_layer = nn.Linear(32, output_dim)\n",
    "\n",
    "    # Called with either one element to determine next action, or a batch\n",
    "    # during optimization. Returns tensor([[left0exp,right0exp]...]).\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self._layer1(x))\n",
    "        x = F.relu(self._layer2(x))\n",
    "        x = F.relu(self._layer3(x))\n",
    "        x = self._output_layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim, output_dim, layer_config=None, non_linearity='relu'):\n",
    "        super(DQN, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.layer_config = self.form_layer_config(layer_config)\n",
    "        self.non_linearity = non_linearity\n",
    "        self.compose_model()\n",
    "\n",
    "    def get_default_config(self):\n",
    "        return [\n",
    "            [self.input_dim, 48],\n",
    "            [48, 32],\n",
    "            [32, 32],\n",
    "            [32, self.output_dim]\n",
    "        ]\n",
    "    \n",
    "    def form_layer_config(self, layer_config):\n",
    "        if layer_config is None:\n",
    "            return self.get_default_config()\n",
    "        \n",
    "        if len(layer_config) < 2:\n",
    "            raise ValueError(\"Layer config must have at least two layers\")\n",
    "\n",
    "        if layer_config[0][0] != self.input_dim:\n",
    "            raise ValueError(\"Input dimension of first layer config must be the same as input to the model\")\n",
    "\n",
    "        if layer_config[-1][1] != self.output_dim:\n",
    "            raise ValueError(\"output dimension of last layer config must be the same as expected model output\")\n",
    "            \n",
    "        for idx in range(len(layer_config) - 1):\n",
    "            assert layer_config[idx][1] == layer_config[idx], \"Dimension mismatch between layers %d and %d\" (idx, idx+1)\n",
    "        \n",
    "        return layer_config\n",
    "\n",
    "    def get_non_linear_class(self):\n",
    "        if self.non_linearity == 'tanh':\n",
    "            return nn.Tanh\n",
    "        else:\n",
    "            return nn.ReLU\n",
    "\n",
    "    def compose_model(self):\n",
    "        non_linear = self.get_non_linear_class()\n",
    "        layers = OrderedDict()\n",
    "        for idx in range(len(self.layer_config)):\n",
    "            input_dim, output_dim = self.layer_config[idx]\n",
    "            layers['linear-%d' % idx] = nn.Linear(input_dim, output_dim)\n",
    "            if idx != len(self.layer_config) - 1:\n",
    "                layers['nonlinear-%d' % idx] = non_linear()\n",
    "        \n",
    "        self.model = nn.Sequential(layers)\n",
    "    \n",
    "    # Called with either one element to determine next action, or a batch\n",
    "    # during optimization. Returns tensor([[left0exp,right0exp]...]).\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "class TaxiAgentUnFancy:\n",
    "    def __init__(self, env, input_dim, output_dim, **kwargs):\n",
    "        self.env = env\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.n_actions = output_dim\n",
    "        self.layer_config = kwargs.get('layer_config', None)\n",
    "        self.batch_size = kwargs.get('batch_size', 1)\n",
    "        self.gamma = kwargs.get('gamma', 0.999)\n",
    "        self.eps_start = kwargs.get('eps_start', 0.9)\n",
    "        self.epsilon = self.eps_start\n",
    "        self.eps_end = kwargs.get('eps_end', 0.05)\n",
    "        self.eps_decay = kwargs.get('eps_decay', 200)\n",
    "        self.target_update = kwargs.get('target_update', 10)\n",
    "        self.replay_capacity = kwargs.get('replay_capacity', 10)\n",
    "        self.non_linearity = kwargs.get('non_linearity', 'relu')\n",
    "        self.optimiser_name = kwargs.get('optimiser_name', 'rmsprop')\n",
    "        self.optimiser_params = kwargs.get('optimiser_params', {})\n",
    "\n",
    "        self.memory = ReplayMemory(self.replay_capacity)\n",
    "\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.steps_done = 0\n",
    "\n",
    "        self.policy_network = DQN(self.input_dim, self.output_dim).to(self.device)\n",
    "        self.target_network = DQN(self.input_dim, self.output_dim).to(self.device)\n",
    "\n",
    "        # we aren't interested in tracking gradients for the target network\n",
    "        self.target_network.load_state_dict(self.policy_network.state_dict())\n",
    "        self.target_network.eval()\n",
    "\n",
    "        self.optimiser = optim.RMSprop(self.policy_network.parameters())\n",
    "\n",
    "        self.state = None\n",
    "        self.reset_env()\n",
    "    \n",
    "    def reset_env(self):\n",
    "        self.env.reset()\n",
    "        self.state = self.state_to_tensor(self.env.s)\n",
    "    \n",
    "    def state_to_tensor(self, state):\n",
    "        if not isinstance(state, Sequence):\n",
    "            state = list(self.env.decode(state))\n",
    "        else:\n",
    "            state = state.copy()\n",
    "        \n",
    "        return torch.tensor(state, device=self.device, dtype=torch.float).reshape(-1, self.input_dim)\n",
    "\n",
    "    def get_optimiser(self):\n",
    "        if self.optimiser_name == 'sgd':\n",
    "            optimiser = optim.RMSprop\n",
    "        elif self.optimiser_name == 'adam':\n",
    "            optimiser = optim.Adam\n",
    "        else:\n",
    "            optimiser = optim.RMSprop\n",
    "        \n",
    "        return optimiser\n",
    "    \n",
    "    def update(self):\n",
    "        if len(self.memory) < self.batch_size:\n",
    "            return\n",
    "        \n",
    "        transitions = self.memory.sample(self.batch_size)\n",
    "\n",
    "        # see https://stackoverflow.com/a/19343/3343043 for detailed explanation\n",
    "        batch = Transition(*zip(*transitions))\n",
    "\n",
    "        loss = self.compute_loss(batch)\n",
    "\n",
    "        self.optimiser.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        for param in self.policy_network.parameters():\n",
    "            param.grad.data.clamp_(-1, 1)\n",
    "        \n",
    "        self.optimiser.step()\n",
    "\n",
    "        return loss\n",
    "    \n",
    "    def double_q_update(self):\n",
    "        self.target_network.load_state_dict(self.policy_network.state_dict())\n",
    "\n",
    "    def compute_loss(self, batch):\n",
    "        non_final_mask = torch.tensor(tuple(map(lambda s: s is not None, \n",
    "            batch.next_state)), device=self.device, dtype=torch.bool)\n",
    "\n",
    "        non_final_next_states = torch.cat([s for s in batch.next_state\n",
    "                                                if s is not None])\n",
    "        state_batch = torch.cat(batch.state)\n",
    "        action_batch = torch.cat(batch.action)\n",
    "        reward_batch = torch.cat(batch.reward)\n",
    "\n",
    "        state_action_values = self.policy_network(state_batch).gather(1, action_batch)\n",
    "        next_state_values = torch.zeros(self.batch_size, device=self.device)\n",
    "\n",
    "\n",
    "        next_state_values[non_final_mask] = self.target_network(non_final_next_states).max(1)[0].detach()\n",
    "        expected_state_action_values = (next_state_values * self.gamma) + reward_batch\n",
    "\n",
    "        loss = F.smooth_l1_loss(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "\n",
    "        return loss\n",
    "    \n",
    "    def select_action(self, state):\n",
    "        sample = random.random()\n",
    "\n",
    "        if sample < self.epsilon:\n",
    "            return torch.tensor([[random.randrange(self.n_actions)]], device=self.device, dtype=torch.long)\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                return self.policy_network.forward(state).max(1)[1].view(1, 1)\n",
    "    \n",
    "\n",
    "    def decay_epsilon(self):\n",
    "        self.epsilon = self.eps_end + (self.eps_start - self.eps_end) * np.exp(-1. * self.steps_done/self.eps_decay)\n",
    "    \n",
    "    def step(self):\n",
    "        state = self.state\n",
    "        self.decay_epsilon()\n",
    "        self.steps_done += 1\n",
    "\n",
    "        action = self.select_action(state)\n",
    "        \n",
    "        next_state, _reward, done, _ = self.env.step(action.item())\n",
    "        next_state = self.state_to_tensor(next_state)\n",
    "        reward = torch.tensor([_reward], device=self.device)\n",
    "\n",
    "        self.memory.push(state, action, next_state, reward)\n",
    "        self.state = next_state\n",
    "\n",
    "        return _reward, done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TaxiAgent:\n",
    "    def __init__(self, env, input_dim, output_dim, **kwargs):\n",
    "        self.env = env\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.n_actions = output_dim\n",
    "        self.layer_config = kwargs.get('layer_config', None)\n",
    "        self.batch_size = kwargs.get('batch_size', 1)\n",
    "        self.gamma = kwargs.get('gamma', 0.999)\n",
    "        self.eps_start = kwargs.get('eps_start', 0.9)\n",
    "        self.epsilon = self.eps_start\n",
    "        self.eps_end = kwargs.get('eps_end', 0.05)\n",
    "        self.eps_decay = kwargs.get('eps_decay', 200)\n",
    "        self.target_update = kwargs.get('target_update', 10)\n",
    "        self.replay_capacity = kwargs.get('replay_capacity', 10)\n",
    "        self.non_linearity = kwargs.get('non_linearity', 'relu')\n",
    "        self.optimiser_name = kwargs.get('optimiser_name', 'rmsprop')\n",
    "        self.optimiser_params = kwargs.get('optimiser_params', {})\n",
    "\n",
    "        self.memory = ReplayMemory(self.replay_capacity)\n",
    "\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.steps_done = 0\n",
    "\n",
    "        self.policy_network = DQN(self.input_dim, self.output_dim, self.layer_config, self.non_linearity).to(self.device)\n",
    "        self.target_network = DQN(self.input_dim, self.output_dim, self.layer_config, self.non_linearity).to(self.device)\n",
    "\n",
    "        # we aren't interested in tracking gradients for the target network\n",
    "        self.target_network.load_state_dict(self.policy_network.state_dict())\n",
    "        self.target_network.eval()\n",
    "\n",
    "        optimiser_cls = self.get_optimiser()\n",
    "        self.optimiser = optimiser_cls(self.policy_network.parameters(), **self.optimiser_params)\n",
    "\n",
    "        self.state = None\n",
    "        self.reset_env()\n",
    "    \n",
    "    def reset_env(self):\n",
    "        self.env.reset()\n",
    "        self.state = self.state_to_tensor(self.env.s)\n",
    "    \n",
    "    def state_to_tensor(self, state):\n",
    "        if not isinstance(state, Sequence):\n",
    "            state = list(self.env.decode(state))\n",
    "        else:\n",
    "            state = state.copy()\n",
    "        \n",
    "        return torch.tensor(state, device=self.device, dtype=torch.float).reshape(-1, self.input_dim)\n",
    "\n",
    "    def get_optimiser(self):\n",
    "        if self.optimiser_name == 'sgd':\n",
    "            optimiser = optim.RMSprop\n",
    "        elif self.optimiser_name == 'adam':\n",
    "            optimiser = optim.Adam\n",
    "        else:\n",
    "            optimiser = optim.RMSprop\n",
    "        \n",
    "        return optimiser\n",
    "    \n",
    "    def update(self):\n",
    "        if len(self.memory) < self.batch_size:\n",
    "            return\n",
    "        \n",
    "        transitions = self.memory.sample(self.batch_size)\n",
    "\n",
    "        # see https://stackoverflow.com/a/19343/3343043 for detailed explanation\n",
    "        batch = Transition(*zip(*transitions))\n",
    "\n",
    "        loss = self.compute_loss(batch)\n",
    "\n",
    "        self.optimiser.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        for param in self.policy_network.parameters():\n",
    "            param.grad.data.clamp_(-1, 1)\n",
    "        \n",
    "        self.optimiser.step()\n",
    "\n",
    "        return loss\n",
    "    \n",
    "    def double_q_update(self):\n",
    "        self.target_network.load_state_dict(self.policy_network.state_dict())\n",
    "\n",
    "    def compute_loss(self, batch):\n",
    "        non_final_mask = torch.tensor(tuple(map(lambda s: s is not None, \n",
    "            batch.next_state)), device=self.device, dtype=torch.bool)\n",
    "\n",
    "        non_final_next_states = torch.cat([s for s in batch.next_state\n",
    "                                                if s is not None])\n",
    "        state_batch = torch.cat(batch.state)\n",
    "        action_batch = torch.cat(batch.action)\n",
    "        reward_batch = torch.cat(batch.reward)\n",
    "\n",
    "        state_action_values = self.policy_network.forward(state_batch).gather(1, action_batch)\n",
    "        next_state_values = torch.zeros(self.batch_size, device=self.device)\n",
    "\n",
    "\n",
    "        next_state_values[non_final_mask] = self.target_network.forward(non_final_next_states).max(1)[0].detach()\n",
    "        expected_state_action_values = (next_state_values * self.gamma) + reward_batch\n",
    "\n",
    "        loss = F.smooth_l1_loss(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "\n",
    "        return loss\n",
    "    \n",
    "    def select_action(self, state):\n",
    "        sample = random.random()\n",
    "\n",
    "        if sample < self.epsilon:\n",
    "            return torch.tensor([[random.randrange(self.n_actions)]], device=self.device, dtype=torch.long)\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                return self.policy_network.forward(state).max(1)[1].view(1, 1)\n",
    "    \n",
    "\n",
    "    def decay_epsilon(self):\n",
    "        self.epsilon = self.eps_end + (self.eps_start - self.eps_end) * np.exp(-1 * self.steps_done/self.eps_decay)\n",
    "    \n",
    "    def step(self):\n",
    "        state = self.state\n",
    "        action = self.select_action(state)\n",
    "\n",
    "        self.steps_done += 1\n",
    "\n",
    "        self.decay_epsilon()\n",
    "        \n",
    "        \n",
    "        next_state, _reward, done, _ = self.env.step(action.item())\n",
    "        next_state = self.state_to_tensor(next_state)\n",
    "        reward = torch.tensor([_reward], device=self.device)\n",
    "\n",
    "        self.memory.push(state, action, next_state, reward)\n",
    "        self.state = next_state\n",
    "\n",
    "        return _reward, done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Bench:\n",
    "    def __init__(self, agent, num_episodes=50):\n",
    "        self.agent = agent\n",
    "        self.rewardList = []\n",
    "        self.episode_duration = []\n",
    "        self.episode_loss = []\n",
    "        self.num_episodes = num_episodes\n",
    "    \n",
    "    def run_trial(self, debug=True):\n",
    "        for idx in range(self.num_episodes):\n",
    "            self.agent.reset_env()\n",
    "            rewards = 0\n",
    "            for jdx in count():\n",
    "                if debug and jdx % 10000 == 0:\n",
    "                    print(\"Running timestep: %d for episode: %d\" % (jdx, idx))\n",
    "                reward, done = self.agent.step()\n",
    "                # optimize\n",
    "                loss = self.agent.update()\n",
    "                self.episode_loss.append(loss)\n",
    "                rewards += reward\n",
    "                if done:\n",
    "                    num_runs = jdx + 1\n",
    "                    self.episode_duration.append(num_runs)\n",
    "                    self.rewardList.append(rewards)\n",
    "                    break\n",
    "            if idx % self.agent.target_update == 0:\n",
    "                self.agent.double_q_update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"Taxi-v3\").env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = 4\n",
    "output_dim = 6\n",
    "agent = TaxiAgent(env, input_dim, output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "bench = Bench(agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running timestep: 0 for episode: 0\n",
      "Running timestep: 0 for episode: 1\n",
      "Running timestep: 10000 for episode: 1\n",
      "Running timestep: 0 for episode: 2\n",
      "Running timestep: 10000 for episode: 2\n",
      "Running timestep: 0 for episode: 3\n",
      "Running timestep: 10000 for episode: 3\n",
      "Running timestep: 20000 for episode: 3\n",
      "Running timestep: 30000 for episode: 3\n",
      "Running timestep: 40000 for episode: 3\n",
      "Running timestep: 50000 for episode: 3\n",
      "Running timestep: 60000 for episode: 3\n",
      "Running timestep: 70000 for episode: 3\n",
      "Running timestep: 80000 for episode: 3\n",
      "Running timestep: 90000 for episode: 3\n",
      "Running timestep: 100000 for episode: 3\n",
      "Running timestep: 110000 for episode: 3\n",
      "Running timestep: 120000 for episode: 3\n",
      "Running timestep: 130000 for episode: 3\n",
      "Running timestep: 140000 for episode: 3\n",
      "Running timestep: 150000 for episode: 3\n",
      "Running timestep: 160000 for episode: 3\n",
      "Running timestep: 170000 for episode: 3\n",
      "Running timestep: 0 for episode: 4\n",
      "Running timestep: 10000 for episode: 4\n",
      "Running timestep: 20000 for episode: 4\n",
      "Running timestep: 30000 for episode: 4\n",
      "Running timestep: 40000 for episode: 4\n",
      "Running timestep: 50000 for episode: 4\n",
      "Running timestep: 60000 for episode: 4\n",
      "Running timestep: 70000 for episode: 4\n",
      "Running timestep: 80000 for episode: 4\n",
      "Running timestep: 90000 for episode: 4\n",
      "Running timestep: 100000 for episode: 4\n",
      "Running timestep: 110000 for episode: 4\n",
      "Running timestep: 120000 for episode: 4\n",
      "Running timestep: 130000 for episode: 4\n",
      "Running timestep: 140000 for episode: 4\n",
      "Running timestep: 150000 for episode: 4\n",
      "Running timestep: 160000 for episode: 4\n",
      "Running timestep: 170000 for episode: 4\n",
      "Running timestep: 180000 for episode: 4\n",
      "Running timestep: 190000 for episode: 4\n",
      "Running timestep: 200000 for episode: 4\n",
      "Running timestep: 210000 for episode: 4\n",
      "Running timestep: 220000 for episode: 4\n",
      "Running timestep: 230000 for episode: 4\n",
      "Running timestep: 240000 for episode: 4\n",
      "Running timestep: 250000 for episode: 4\n",
      "Running timestep: 0 for episode: 5\n",
      "Running timestep: 10000 for episode: 5\n",
      "Running timestep: 20000 for episode: 5\n",
      "Running timestep: 30000 for episode: 5\n",
      "Running timestep: 40000 for episode: 5\n",
      "Running timestep: 50000 for episode: 5\n",
      "Running timestep: 60000 for episode: 5\n",
      "Running timestep: 70000 for episode: 5\n",
      "Running timestep: 80000 for episode: 5\n",
      "Running timestep: 90000 for episode: 5\n",
      "Running timestep: 100000 for episode: 5\n",
      "Running timestep: 110000 for episode: 5\n",
      "Running timestep: 120000 for episode: 5\n",
      "Running timestep: 130000 for episode: 5\n",
      "Running timestep: 140000 for episode: 5\n",
      "Running timestep: 150000 for episode: 5\n",
      "Running timestep: 160000 for episode: 5\n",
      "Running timestep: 170000 for episode: 5\n",
      "Running timestep: 180000 for episode: 5\n",
      "Running timestep: 190000 for episode: 5\n",
      "Running timestep: 200000 for episode: 5\n",
      "Running timestep: 0 for episode: 6\n",
      "Running timestep: 10000 for episode: 6\n",
      "Running timestep: 20000 for episode: 6\n",
      "Running timestep: 30000 for episode: 6\n",
      "Running timestep: 0 for episode: 7\n",
      "Running timestep: 10000 for episode: 7\n",
      "Running timestep: 20000 for episode: 7\n",
      "Running timestep: 30000 for episode: 7\n",
      "Running timestep: 40000 for episode: 7\n",
      "Running timestep: 50000 for episode: 7\n",
      "Running timestep: 60000 for episode: 7\n",
      "Running timestep: 70000 for episode: 7\n",
      "Running timestep: 80000 for episode: 7\n",
      "Running timestep: 0 for episode: 8\n",
      "Running timestep: 10000 for episode: 8\n",
      "Running timestep: 20000 for episode: 8\n",
      "Running timestep: 0 for episode: 9\n",
      "Running timestep: 0 for episode: 10\n",
      "Running timestep: 0 for episode: 11\n",
      "Running timestep: 0 for episode: 12\n",
      "Running timestep: 0 for episode: 13\n",
      "Running timestep: 10000 for episode: 13\n",
      "Running timestep: 0 for episode: 14\n",
      "Running timestep: 0 for episode: 15\n",
      "Running timestep: 0 for episode: 16\n",
      "Running timestep: 0 for episode: 17\n",
      "Running timestep: 0 for episode: 18\n",
      "Running timestep: 10000 for episode: 18\n",
      "Running timestep: 20000 for episode: 18\n",
      "Running timestep: 30000 for episode: 18\n",
      "Running timestep: 0 for episode: 19\n",
      "Running timestep: 0 for episode: 20\n",
      "Running timestep: 0 for episode: 21\n",
      "Running timestep: 10000 for episode: 21\n",
      "Running timestep: 0 for episode: 22\n",
      "Running timestep: 0 for episode: 23\n",
      "Running timestep: 0 for episode: 24\n",
      "Running timestep: 0 for episode: 25\n",
      "Running timestep: 10000 for episode: 25\n",
      "Running timestep: 0 for episode: 26\n",
      "Running timestep: 10000 for episode: 26\n",
      "Running timestep: 20000 for episode: 26\n",
      "Running timestep: 30000 for episode: 26\n",
      "Running timestep: 0 for episode: 27\n",
      "Running timestep: 0 for episode: 28\n",
      "Running timestep: 0 for episode: 29\n",
      "Running timestep: 0 for episode: 30\n",
      "Running timestep: 0 for episode: 31\n",
      "Running timestep: 10000 for episode: 31\n",
      "Running timestep: 20000 for episode: 31\n",
      "Running timestep: 0 for episode: 32\n",
      "Running timestep: 10000 for episode: 32\n",
      "Running timestep: 0 for episode: 33\n",
      "Running timestep: 0 for episode: 34\n",
      "Running timestep: 0 for episode: 35\n",
      "Running timestep: 10000 for episode: 35\n",
      "Running timestep: 20000 for episode: 35\n",
      "Running timestep: 30000 for episode: 35\n",
      "Running timestep: 40000 for episode: 35\n",
      "Running timestep: 50000 for episode: 35\n",
      "Running timestep: 0 for episode: 36\n",
      "Running timestep: 0 for episode: 37\n",
      "Running timestep: 10000 for episode: 37\n",
      "Running timestep: 0 for episode: 38\n",
      "Running timestep: 0 for episode: 39\n",
      "Running timestep: 0 for episode: 40\n",
      "Running timestep: 10000 for episode: 40\n",
      "Running timestep: 0 for episode: 41\n",
      "Running timestep: 0 for episode: 42\n",
      "Running timestep: 0 for episode: 43\n",
      "Running timestep: 0 for episode: 44\n",
      "Running timestep: 10000 for episode: 44\n",
      "Running timestep: 20000 for episode: 44\n",
      "Running timestep: 30000 for episode: 44\n",
      "Running timestep: 0 for episode: 45\n",
      "Running timestep: 10000 for episode: 45\n",
      "Running timestep: 20000 for episode: 45\n",
      "Running timestep: 30000 for episode: 45\n",
      "Running timestep: 0 for episode: 46\n",
      "Running timestep: 0 for episode: 47\n",
      "Running timestep: 10000 for episode: 47\n",
      "Running timestep: 20000 for episode: 47\n",
      "Running timestep: 30000 for episode: 47\n",
      "Running timestep: 40000 for episode: 47\n",
      "Running timestep: 0 for episode: 48\n",
      "Running timestep: 0 for episode: 49\n"
     ]
    }
   ],
   "source": [
    "bench.run_trial()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build up the q-table using the network!\n",
    "q_table = np.zeros((env.nS, env.nA))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    for idx in range(env.nS):\n",
    "        s = agent.state_to_tensor(idx)\n",
    "        q_values = agent.policy_network(s)\n",
    "        q_values = q_values.detach().numpy()\n",
    "        q_table[idx] = q_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ -5.35347414,  -5.3162179 ,  -5.30059767,  -5.30166054,\n",
       "       -14.45737267, -14.48056984])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_table[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = bench.episode_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = [_.item() for _ in bench.episode_loss]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1a27638080>]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(range(len(losses)), losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
