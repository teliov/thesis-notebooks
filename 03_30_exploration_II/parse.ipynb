{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook parses two dask implementations: groupby and foldby. Comparisons are made in terms of memory consumption and CPU run time. \n",
    "\n",
    "Also we progressively increase the data size to see what the effects of increased data size are on the performance of the respective algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "# So we can use the *thesislib* package\n",
    "import sys\n",
    "import os\n",
    "\n",
    "module_path = os.path.abspath(\"..\")\n",
    "\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from thesislib.utils import pathutils\n",
    "from glob import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_directory = pathutils.get_data_file(\"exploration_II/output\")\n",
    "\n",
    "if not os.path.isdir(output_directory):\n",
    "    os.mkdir(output_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_condition_symptom_from_modules(module_name):\n",
    "    with open(module_name) as fp:\n",
    "        module = json.load(fp)\n",
    "    condition_code = None\n",
    "    condition_name = None\n",
    "    symptoms = {}\n",
    "    \n",
    "    states = module.get('states')\n",
    "    for state in states.values():\n",
    "        state_type = state.get('type')\n",
    "        if state_type == 'ConditionOnset':\n",
    "            condition_code = state.get('codes')[0].get('code')\n",
    "            condition_name = state.get('codes')[0].get('display')\n",
    "        elif state_type == 'Symptom':\n",
    "            symptom_code = state.get('symptom_code').get('code')\n",
    "            symptom_name = state.get('symptom')\n",
    "            symptoms[symptom_code] = symptom_name\n",
    "    return condition_code, condition_name, symptoms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# symptom db and conditions db files\n",
    "symptom_db_json = pathutils.get_data_file(\"exploration_II/output/symptom_db.json\")\n",
    "condition_db_json = pathutils.get_data_file(\"exploration_II/output/condition_db.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isfile(symptom_db_json) or not os.path.isfile(condition_db_json):\n",
    "    module_files = glob(pathutils.get_data_file(\"exploration_II/data/all_modules/*.json\"))\n",
    "    symptom_db = {}\n",
    "    condition_db = {}\n",
    "    \n",
    "    for file in module_files:\n",
    "        c_code, c_name, syms = extract_condition_symptom_from_modules(file)\n",
    "        condition_db[c_code] = c_name\n",
    "        symptom_db.update(syms)\n",
    "    with open(symptom_db_json, \"w\") as fp:\n",
    "        json.dump(symptom_db, fp)\n",
    "    with open(condition_db_json, \"w\") as fp:\n",
    "        json.dump(condition_db, fp)\n",
    "else:\n",
    "    with open(symptom_db_json) as fp:\n",
    "        symptom_db = json.load(fp)\n",
    "    with open(condition_db_json) as fp:\n",
    "        condition_db = json.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "symptom_vector = set(symptom_db.keys())\n",
    "condition_codes = set(condition_db.keys())\n",
    "condition_labels = {code: idx for idx, code in enumerate(condition_codes)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "patients_csv = pathutils.get_data_file(\"exploration_II/data/new/patients.csv\")\n",
    "conditions_csv = pathutils.get_data_file(\"exploration_II/data/new/conditions.csv\")\n",
    "symptoms_csv = pathutils.get_data_file(\"exploration_II/data/new/symptoms.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table style=\"border: 2px solid white;\">\n",
       "<tr>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Client</h3>\n",
       "<ul style=\"text-align: left; list-style: none; margin: 0; padding: 0;\">\n",
       "  <li><b>Scheduler: </b>tcp://127.0.0.1:63669</li>\n",
       "  <li><b>Dashboard: </b><a href='http://127.0.0.1:8787/status' target='_blank'>http://127.0.0.1:8787/status</a>\n",
       "</ul>\n",
       "</td>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Cluster</h3>\n",
       "<ul style=\"text-align: left; list-style:none; margin: 0; padding: 0;\">\n",
       "  <li><b>Workers: </b>2</li>\n",
       "  <li><b>Cores: </b>4</li>\n",
       "  <li><b>Memory: </b>8.00 GB</li>\n",
       "</ul>\n",
       "</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<Client: 'tcp://127.0.0.1:63669' processes=2 threads=4, memory=8.00 GB>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dask.distributed import Client, progress\n",
    "client = Client(n_workers=2, threads_per_worker=2, memory_limit='4GB')\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "patient_columns = ['Id', 'BIRTHDATE', 'RACE', 'GENDER']\n",
    "condition_columns = ['ENCOUNTER', 'PATIENT', 'CODE', 'START']\n",
    "symptom_colums = ['ENCOUNTER', 'PATIENT', 'SYMPTOM_CODE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/teliov/Library/anaconda3/envs/ml/lib/python3.6/site-packages/fsspec/implementations/local.py:33: FutureWarning: The default value of auto_mkdir=True has been deprecated and will be changed to auto_mkdir=False by default in a future release.\n",
      "  FutureWarning,\n"
     ]
    }
   ],
   "source": [
    "# let's try the group by first\n",
    "\n",
    "patients = dd.read_csv(\n",
    "    patients_csv,\n",
    "    usecols=patient_columns,\n",
    "    parse_dates=['BIRTHDATE'],\n",
    "    infer_datetime_format=True,\n",
    "    dtype={\n",
    "        'GENDER': 'category',\n",
    "        'RACE': 'category'\n",
    "    }\n",
    ")\n",
    "\n",
    "\n",
    "conditions = dd.read_csv(conditions_csv, usecols=condition_columns, parse_dates=['START'], infer_datetime_format=True)\n",
    "\n",
    "\n",
    "symptoms = dd.read_csv(symptoms_csv, usecols=symptom_colums)\n",
    "symptoms = symptoms.repartition(npartitions=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _race_txform(val):\n",
    "    race_code = {'white': 0, 'black':1, 'asian':2, 'native':3, 'other':4}\n",
    "    return race_code.get(val)\n",
    "def _label_txform(val, labels):\n",
    "    return labels.get(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patients['RACE'] = patients['RACE'].apply(_race_txform, meta=('RACE', np.uint8))\n",
    "patients['GENDER'] = patients['GENDER'].apply(lambda gender: 0 if gender == 'F' else 1, meta=('GENDER', np.bool))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "conditions['LABEL'] = conditions['CODE'].apply(_label_txform, labels=condition_labels, meta=('CODE', np.uint16))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = conditions.merge(patients, how='left', left_on='PATIENT', right_on='Id', suffixes=('', '_pat'))\n",
    "df = symptoms.merge(df, how='left', left_on='ENCOUNTER', right_on='ENCOUNTER', suffixes=('_symp', ''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['AGE'] = ((df['START'] - df['BIRTHDATE']).astype('timedelta64[M]')/12).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interest_keys = {'LABEL', 'RACE', 'GENDER', 'AGE', 'ENCOUNTER', 'SYMPTOM_CODE'}\n",
    "key_map = {}\n",
    "for idx, column in enumerate(df.columns):\n",
    "    if column in interest_keys:\n",
    "        key_map[idx] = column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "parts = ['ENCOUNTER', 'LABEL', 'RACE', 'GENDER', 'AGE']\n",
    "label_map = OrderedDict({itm: idx for idx, itm in enumerate(parts)})\n",
    "symptom_start = symptom_idx =  len(parts)\n",
    "for item in sorted(symptom_vector):\n",
    "    label_map[item] = symptom_idx\n",
    "    symptom_idx += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# enforce the order of the keys, and select just those we want\n",
    "ordered_keys = parts + ['SYMPTOM_CODE']\n",
    "df = df[ordered_keys]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change to a bag because we're going to do some non dataframe friendly operations\n",
    "bag = df.to_bag()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# these are the columns in the order we want them:\n",
    "# Index(['ENCOUNTER', 'LABEL', 'RACE', 'GENDER', 'AGE', 'SYMPTOM_CODE'], dtype='object')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we need to append the index of the symptom in the final reduction so we can pick it up \n",
    "def append_index_to_symptom(row, label_map):\n",
    "    val = list(row)\n",
    "    symptom = val[5] # we know it would be the fifth\n",
    "    idx = label_map[symptom]\n",
    "    val[5] = idx\n",
    "    return val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_bag = bag.map(append_index_to_symptom, label_map=label_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_reduce(item, num_columns):\n",
    "    encounter, rows = item\n",
    "    reduction =n  ['0' for idx in range(num_columns)]\n",
    "    for row in rows:\n",
    "        for idx, val in enumerate(row):\n",
    "            if idx <= 4:\n",
    "                reduction[idx] = str(val)\n",
    "            else:\n",
    "                reduction[val] = '1'\n",
    "    return encounter, reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# group by the first coolum i.e the encounter\n",
    "grouped = t_bag.groupby(lambda v: v[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_columns = len(label_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# and the reduction ??\n",
    "reduced_w_keys = grouped.map(do_reduce, num_columns=num_columns)\n",
    "reduced = reduced_w_keys.map(lambda v: v[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get it into a string so we can put it in text files\n",
    "text_reduced = reduced.map(lambda v: \",\".join(v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/Users/teliov/TUD/Thesis/Medvice/Notebooks/data/exploration_II/output/parsed/data-0.csv',\n",
       " '/Users/teliov/TUD/Thesis/Medvice/Notebooks/data/exploration_II/output/parsed/data-1.csv',\n",
       " '/Users/teliov/TUD/Thesis/Medvice/Notebooks/data/exploration_II/output/parsed/data-2.csv',\n",
       " '/Users/teliov/TUD/Thesis/Medvice/Notebooks/data/exploration_II/output/parsed/data-3.csv']"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# output dir\n",
    "csv_output = pathutils.get_data_file(\"exploration_II/output/parsed/data-*.csv\")\n",
    "op_files = text_reduced.to_textfiles(csv_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_names = list(label_map.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "d1 = pd.read_csv('/Users/teliov/TUD/Thesis/Medvice/Notebooks/data/exploration_II/output/parsed/data-0.csv', names=column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "symp_pd = pd.read_csv(symptoms_csv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Very slow comparison\n",
    "- did we get it right?\n",
    "- compare read results, make sure the right symptoms were encoded!\n",
    "```python\n",
    "for val in d1.itertuples():\n",
    "    encounterId = val[1]\n",
    "    recorded_symptoms = []\n",
    "    for idx, item in enumerate(val[6:]):\n",
    "        if item == 1:\n",
    "            recorded_symptoms.append(column_names[idx + 5])\n",
    "    # get the symptoms that were present previously\n",
    "    encounter_symptoms = symp_pd[symp_pd.ENCOUNTER == encounterId].SYMPTOM_CODE\n",
    "    num_recorded_symptoms = len(recorded_symptoms)\n",
    "    num_actual_symptoms = len(encounter_symptoms)\n",
    "    assert num_recorded_symptoms == num_actual_symptoms, \"Expected %d but got %d symptoms for %s\" % (num_actual_symptoms, num_recorded_symptoms, encounterId)\n",
    "    \n",
    "    # make sure we have the same set\n",
    "    recorded_symptoms = set(recorded_symptoms)\n",
    "    encounter_symptoms = set(encounter_symptoms)\n",
    "    same = recorded_symptoms & encounter_symptoms\n",
    "    num_similar = len(same)\n",
    "    assert num_similar == num_actual_symptoms, \"Expected both recorded symptoms and actual symptoms to be the same\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# can we make the test faster !?\n",
    "np_column_names = np.array(column_names)\n",
    "grouped_symptoms = symp_pd.groupby('ENCOUNTER')\n",
    "for val in d1.itertuples():\n",
    "    encounterId = val[1]\n",
    "    recorded_symptoms = np_column_names[np.nonzero(val[6:])[0] + 5]\n",
    "    encounter_symptoms = grouped_symptoms.get_group(encounterId).SYMPTOM_CODE\n",
    "    same_length = len(recorded_symptoms) == len(encounter_symptoms)\n",
    "    same_elem = set(recorded_symptoms) == set(encounter_symptoms)\n",
    "    assert same_length and same_elem, \"Expected both to be the same\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we know now that this method words. And it seemed much faster than previous attempts. \n",
    "\n",
    "Memory consumption was much better and it was (in my opinion) almost as fast if not faster than the pd approach\n",
    "\n",
    "Can we try the foldBy approach? Would it be better (since it's supposed to avoid expensive operations as per the dask documentation)\n",
    "\n",
    "We'll start all over so we get a fair comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "# and now for foldby\n",
    "# let's try the group by first\n",
    "fpatients = dd.read_csv(\n",
    "    patients_csv,\n",
    "    usecols=patient_columns,\n",
    "    parse_dates=['BIRTHDATE'],\n",
    "    infer_datetime_format=True,\n",
    "    dtype={\n",
    "        'GENDER': 'category',\n",
    "        'RACE': 'category'\n",
    "    }\n",
    ")\n",
    "\n",
    "fconditions = dd.read_csv(conditions_csv, usecols=condition_columns, parse_dates=['START'], infer_datetime_format=True)\n",
    "\n",
    "fsymptoms = dd.read_csv(symptoms_csv, usecols=symptom_colums)\n",
    "fsymptoms = fsymptoms.repartition(npartitions=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpatients['RACE'] = fpatients['RACE'].apply(_race_txform, meta=('RACE', np.uint8))\n",
    "fpatients['GENDER'] = fpatients['GENDER'].apply(lambda gender: 0 if gender == 'F' else 1, meta=('GENDER', np.bool))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "fconditions['LABEL'] = conditions['CODE'].apply(_label_txform, labels=condition_labels, meta=('CODE', np.uint16))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "fdf = fconditions.merge(fpatients, how='left', left_on='PATIENT', right_on='Id', suffixes=('', '_pat'))\n",
    "fdf = fsymptoms.merge(fdf, how='left', left_on='ENCOUNTER', right_on='ENCOUNTER', suffixes=('_symp', ''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "fdf['AGE'] = ((fdf['START'] - fdf['BIRTHDATE']).astype('timedelta64[M]')/12).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform the symptom codes as well\n",
    "def transform_symptom_codes(item, label_map):\n",
    "    return label_map.get(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "fdf['SYMPTOM_CODE'] = fdf['SYMPTOM_CODE'].apply(transform_symptom_codes, label_map=label_map, meta=('SYMPTOM_CODE', np.uint16))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "# force the column order\n",
    "fdf = fdf[ordered_keys]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['ENCOUNTER', 'LABEL', 'RACE', 'GENDER', 'AGE', 'SYMPTOM_CODE'], dtype='object')"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fdf.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_entry = ['0' for idx in range(num_columns)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bin_op(v1, v2):\n",
    "    # Index(['ENCOUNTER', 'LABEL', 'RACE', 'GENDER', 'AGE', 'SYMPTOM_CODE'], dtype='object')\n",
    "    if v1[0] == '0':\n",
    "        base = list(v1)\n",
    "        init = True\n",
    "    else:\n",
    "        base = v1\n",
    "        init = False\n",
    "    if init:\n",
    "        for idx in range(5):\n",
    "            base[idx] = str(v2[idx])\n",
    "    base[v2[5]] = '1'\n",
    "    return base\n",
    "\n",
    "def combine_op(v1, v2):\n",
    "    for idx, item in enumerate(v2[5:]):\n",
    "        if item == '1':\n",
    "            v1[idx + 5] = '1'\n",
    "    return v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "fbag = fdf.to_bag()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "folded_w_key= fbag.foldby(lambda v: v[0], binop=bin_op, initial=initial_entry, combine=combine_op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_folded = folded_w_key.map(lambda v: \",\".join(v[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output dir\n",
    "fcsv_output = pathutils.get_data_file(\"exploration_II/output/parsed_foldby/data-*.csv\")\n",
    "op_files = text_folded.to_textfiles(fcsv_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "fd1 = pd.read_csv(op_files[0], names=column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "# is it correct!\n",
    "for val in fd1.itertuples():\n",
    "    encounterId = val[1]\n",
    "    recorded_symptoms = np_column_names[np.nonzero(val[6:])[0] + 5]\n",
    "    encounter_symptoms = grouped_symptoms.get_group(encounterId).SYMPTOM_CODE\n",
    "    same_length = len(recorded_symptoms) == len(encounter_symptoms)\n",
    "    same_elem = set(recorded_symptoms) == set(encounter_symptoms)\n",
    "    assert same_length and same_elem, \"Expected both to be the same\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Foldby was blazing fast!!! \n",
    "\n",
    "I'll add a timer for quantitative evaluation. But this was incredible speed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Update**\n",
    "\n",
    "Sadly these methods though they work well on the LocalCluster, they seem to fail in the distributed scheduler. \n",
    "\n",
    "I am going to try for a dataframe only approach and see. I have an idea."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/teliov/Library/anaconda3/envs/ml/lib/python3.6/site-packages/fsspec/implementations/local.py:33: FutureWarning: The default value of auto_mkdir=True has been deprecated and will be changed to auto_mkdir=False by default in a future release.\n",
      "  FutureWarning,\n"
     ]
    }
   ],
   "source": [
    "# let's start all over.\n",
    "dfpatients = dd.read_csv(\n",
    "    patients_csv,\n",
    "    usecols=patient_columns,\n",
    "    parse_dates=['BIRTHDATE'],\n",
    "    infer_datetime_format=True,\n",
    ")\n",
    "\n",
    "dfconditions = dd.read_csv(conditions_csv, usecols=condition_columns, parse_dates=['START'], infer_datetime_format=True)\n",
    "\n",
    "dfsymptoms = dd.read_csv(symptoms_csv, usecols=symptom_colums)\n",
    "dfsymptoms = dfsymptoms.repartition(npartitions=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfpatients['RACE'] = dfpatients['RACE'].apply(_race_txform, meta=('RACE', np.uint8))\n",
    "dfpatients['GENDER'] = dfpatients['GENDER'].apply(lambda gender: 0 if gender == 'F' else 1, meta=('GENDER', np.bool))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfconditions['LABEL'] = dfconditions['CODE'].apply(_label_txform, labels=condition_labels, meta=('CODE', np.uint16))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfsymptoms['SYMPTOM_CODE'] = dfsymptoms['SYMPTOM_CODE'].apply(transform_symptom_codes, label_map=label_map, meta=('SYMPTOM_CODE', np.uint16))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "comb = dfconditions.merge(dfpatients, how='left', left_on='PATIENT', right_on='Id', suffixes=('', '_pat'))\n",
    "comb = dfsymptoms.merge(comb, how='left', left_on='ENCOUNTER', right_on='ENCOUNTER', suffixes=('_symp', ''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "comb['AGE'] = ((comb['START'] - comb['BIRTHDATE']).astype('timedelta64[M]')/12).astype(np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "ordered_keys = parts + ['SYMPTOM_CODE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "comb = comb[ordered_keys]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "comb['counter'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_keys = list(label_map.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# need to grow the data frame\n",
    "for idx in range(5, 381):\n",
    "    comb[label_keys[idx]] = (comb.SYMPTOM_CODE == idx).astype(np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# group by the encounterId\n",
    "comb_grouped = comb.groupby('ENCOUNTER')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "sumed = comb_grouped.agg('sum')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "sumed['LABEL'] = (sumed['LABEL']/sumed['counter']).astype(np.uint16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "sumed['RACE'] = (sumed['RACE']/sumed['counter']).astype(np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "sumed['AGE'] = (sumed['AGE']/sumed['counter']).astype(np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "sumed['GENDER'] = (sumed['GENDER']/sumed['counter']).astype(np.bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/Users/teliov/TUD/Thesis/Medvice/Notebooks/data/exploration_II/output/parsed_foldby/dfdata-0.csv']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# and now we should have what we want ...\n",
    "# but we got into the one dataframe issue again. And it'll be tricky to verify the results because we also lost the condition index. \n",
    "# but let's see ..\n",
    "dffcsv_output = pathutils.get_data_file(\"exploration_II/output/parsed_foldby/dfdata-*.csv\")\n",
    "sumed.to_csv(dffcsv_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This was quite fast too!\n",
    "# but wwe also got the 1-partition business, so I don't know how this works on the cluster \n",
    "# but first let's see if this is correct!\n",
    "symp_pd = pd.read_csv(symptoms_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfsoln = pd.read_csv('/Users/teliov/TUD/Thesis/Medvice/Notebooks/data/exploration_II/output/parsed_foldby/dfdata-0.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_names = list(label_map.keys())\n",
    "np_column_names = np.array(column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_symptoms = symp_pd.groupby('ENCOUNTER')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "for val in dfsoln.itertuples():\n",
    "    encounterId = val[1]\n",
    "    recorded_symptoms = np_column_names[np.nonzero(val[8:])[0] + 5]\n",
    "    encounter_symptoms = grouped_symptoms.get_group(encounterId).SYMPTOM_CODE\n",
    "    same_length = len(recorded_symptoms) == len(encounter_symptoms)\n",
    "    same_elem = set(recorded_symptoms) == set(encounter_symptoms)\n",
    "    assert same_length and same_elem, \"Expected both to be the same\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Update**\n",
    "\n",
    "The dataframe solution has one huge problem: memory explosion!\n",
    "\n",
    "When you think about it, I am expanding every symptom, every single one of them only to sum them up in the end. \n",
    "\n",
    "If I could avoid this expansion and do it after grouping, then that would be great. \n",
    "\n",
    "It would of course require a different kind of symptom encoding so that I can use bit wise operators to recover the actual symptoms that were combined.\n",
    "\n",
    "Memory conservation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [],
   "source": [
    "from thesislib.utils import pathutils\n",
    "from glob import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_directory = pathutils.get_data_file(\"exploration_II/output\")\n",
    "\n",
    "if not os.path.isdir(output_directory):\n",
    "    os.mkdir(output_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_condition_symptom_from_modules(module_name):\n",
    "    with open(module_name) as fp:\n",
    "        module = json.load(fp)\n",
    "    condition_code = None\n",
    "    condition_name = None\n",
    "    symptoms = {}\n",
    "    \n",
    "    states = module.get('states')\n",
    "    for state in states.values():\n",
    "        state_type = state.get('type')\n",
    "        if state_type == 'ConditionOnset':\n",
    "            condition_code = state.get('codes')[0].get('code')\n",
    "            condition_name = state.get('codes')[0].get('display')\n",
    "        elif state_type == 'Symptom':\n",
    "            symptom_code = state.get('symptom_code').get('code')\n",
    "            symptom_name = state.get('symptom')\n",
    "            symptoms[symptom_code] = symptom_name\n",
    "    return condition_code, condition_name, symptoms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [],
   "source": [
    "# symptom db and conditions db files\n",
    "symptom_db_json = pathutils.get_data_file(\"exploration_II/output/symptom_db.json\")\n",
    "condition_db_json = pathutils.get_data_file(\"exploration_II/output/condition_db.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isfile(symptom_db_json) or not os.path.isfile(condition_db_json):\n",
    "    module_files = glob(pathutils.get_data_file(\"exploration_II/data/all_modules/*.json\"))\n",
    "    symptom_db = {}\n",
    "    condition_db = {}\n",
    "    \n",
    "    for file in module_files:\n",
    "        c_code, c_name, syms = extract_condition_symptom_from_modules(file)\n",
    "        condition_db[c_code] = c_name\n",
    "        symptom_db.update(syms)\n",
    "    with open(symptom_db_json, \"w\") as fp:\n",
    "        json.dump(symptom_db, fp)\n",
    "    with open(condition_db_json, \"w\") as fp:\n",
    "        json.dump(condition_db, fp)\n",
    "else:\n",
    "    with open(symptom_db_json) as fp:\n",
    "        symptom_db = json.load(fp)\n",
    "    with open(condition_db_json) as fp:\n",
    "        condition_db = json.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {},
   "outputs": [],
   "source": [
    "symptom_vector = sorted(symptom_db.keys())\n",
    "condition_codes = sorted(condition_db.keys())\n",
    "condition_labels = {code: idx for idx, code in enumerate(condition_codes)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {},
   "outputs": [],
   "source": [
    "patients_csv = pathutils.get_data_file(\"exploration_II/data/new/patients.csv\")\n",
    "conditions_csv = pathutils.get_data_file(\"exploration_II/data/new/conditions.csv\")\n",
    "symptoms_csv = pathutils.get_data_file(\"exploration_II/data/new/symptoms.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/teliov/Library/anaconda3/envs/ml/lib/python3.6/site-packages/distributed/dashboard/core.py:79: UserWarning: \n",
      "Port 8787 is already in use. \n",
      "Perhaps you already have a cluster running?\n",
      "Hosting the diagnostics dashboard on a random port instead.\n",
      "  warnings.warn(\"\\n\" + msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table style=\"border: 2px solid white;\">\n",
       "<tr>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Client</h3>\n",
       "<ul style=\"text-align: left; list-style: none; margin: 0; padding: 0;\">\n",
       "  <li><b>Scheduler: </b>tcp://127.0.0.1:63640</li>\n",
       "  <li><b>Dashboard: </b><a href='http://127.0.0.1:63641/status' target='_blank'>http://127.0.0.1:63641/status</a>\n",
       "</ul>\n",
       "</td>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Cluster</h3>\n",
       "<ul style=\"text-align: left; list-style:none; margin: 0; padding: 0;\">\n",
       "  <li><b>Workers: </b>2</li>\n",
       "  <li><b>Cores: </b>4</li>\n",
       "  <li><b>Memory: </b>8.00 GB</li>\n",
       "</ul>\n",
       "</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<Client: 'tcp://127.0.0.1:63640' processes=2 threads=4, memory=8.00 GB>"
      ]
     },
     "execution_count": 377,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dask.distributed import Client, progress\n",
    "client = Client(n_workers=2, threads_per_worker=2, memory_limit='4GB')\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {},
   "outputs": [],
   "source": [
    "patient_columns = ['Id', 'BIRTHDATE', 'RACE', 'GENDER']\n",
    "condition_columns = ['ENCOUNTER', 'PATIENT', 'CODE', 'START']\n",
    "symptom_colums = ['ENCOUNTER', 'PATIENT', 'SYMPTOM_CODE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _race_txform(val):\n",
    "    race_code = {'white': 0, 'black':1, 'asian':2, 'native':3, 'other':4}\n",
    "    return race_code.get(val)\n",
    "def _label_txform(val, labels):\n",
    "    return labels.get(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's start all over.\n",
    "df2patients = dd.read_csv(\n",
    "    patients_csv,\n",
    "    usecols=patient_columns,\n",
    "    parse_dates=['BIRTHDATE'],\n",
    "    infer_datetime_format=True,\n",
    ")\n",
    "\n",
    "df2conditions = dd.read_csv(conditions_csv, usecols=condition_columns, parse_dates=['START'], infer_datetime_format=True)\n",
    "\n",
    "df2symptoms = dd.read_csv(symptoms_csv, usecols=symptom_colums)\n",
    "df2symptoms = df2symptoms.repartition(npartitions=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2patients['RACE'] = df2patients['RACE'].apply(_race_txform, meta=('RACE', np.uint8))\n",
    "df2patients['GENDER'] = df2patients['GENDER'].apply(lambda gender: 0 if gender == 'F' else 1, meta=('GENDER', np.bool))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2conditions['LABEL'] = df2conditions['CODE'].apply(_label_txform, labels=condition_labels, meta=('CODE', np.uint16))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df2conditions.merge(df2patients, how='left', left_on='PATIENT', right_on='Id', suffixes=('', '_pat'))\n",
    "df2 = df2symptoms.merge(df2, how='left', left_on='ENCOUNTER', right_on='ENCOUNTER', suffixes=('_symp', ''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2['AGE'] = ((df2['START'] - df2['BIRTHDATE']).astype('timedelta64[M]')/12).astype(np.uint16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {},
   "outputs": [],
   "source": [
    "parts = ['ENCOUNTER', 'LABEL', 'RACE', 'GENDER', 'AGE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map = OrderedDict({})\n",
    "power = np.array([2**idx for idx in range(len(symptom_vector))])\n",
    "for idx, item in enumerate(symptom_vector):\n",
    "    label_map[item] = power[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {},
   "outputs": [],
   "source": [
    "# enforce the order of the keys, and select just those we want\n",
    "ordered_keys = parts + ['SYMPTOM_CODE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_columns = len(symptom_vector) + len(parts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform the symptom codes as well\n",
    "def transform_symptom_codes(item, label_map):\n",
    "    return label_map.get(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp = np.array([2**376])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2symptoms['SYMPTOM_CODE'] = df2symptoms['SYMPTOM_CODE'].apply(transform_symptom_codes, label_map=label_map, meta=('SYMPTOM_CODE', power.dtype))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {},
   "outputs": [],
   "source": [
    "comb = df2conditions.merge(df2patients, how='left', left_on='PATIENT', right_on='Id', suffixes=('', '_pat'))\n",
    "comb = df2symptoms.merge(comb, how='left', left_on='ENCOUNTER', right_on='ENCOUNTER', suffixes=('_symp', ''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {},
   "outputs": [],
   "source": [
    "comb['AGE'] = ((comb['START'] - comb['BIRTHDATE']).astype('timedelta64[M]')/12).astype(np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {},
   "outputs": [],
   "source": [
    "comb = comb[ordered_keys]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {},
   "outputs": [],
   "source": [
    "comb['counter'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {},
   "outputs": [],
   "source": [
    "# group by the encounterId\n",
    "comb_grouped = comb.groupby('ENCOUNTER')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {},
   "outputs": [],
   "source": [
    "sumed = comb_grouped.agg('sum', split_out=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtypes = {\n",
    "    'LABEL': np.uint16,\n",
    "    'RACE': np.uint8,\n",
    "    'AGE': np.uint8,\n",
    "    'GENDER': np.uint8,\n",
    "    'SYMPTOM_CODE': np.object\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_agg(df):\n",
    "    df.LABEL = (df.LABEL/df.counter).astype(np.uint16)\n",
    "    df.RACE = (df.RACE/df.counter).astype(np.uint8)\n",
    "    df.AGE = (df.AGE/df.counter).astype(np.uint8)\n",
    "    df.GENDER = (df.GENDER/df.counter).astype(np.uint8)\n",
    "    return df[['LABEL', 'RACE', 'AGE', 'GENDER', 'SYMPTOM_CODE']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {},
   "outputs": [],
   "source": [
    "sumed = sumed.map_partitions(map_agg, meta=dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check(val, comp):\n",
    "    c = val & comp\n",
    "    if c > 0:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_expand(df, label_map):\n",
    "    def check_inner(val, comp):\n",
    "        c = val & comp\n",
    "        if c > 0:\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "    for k, v in label_map.items():\n",
    "        df[k] = df.SYMPTOM_CODE.apply(check_inner, comp=v)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_dtype = {item: val for item, val in dtypes.items()}\n",
    "full_dtype.update({item: np.uint8 for item in label_map})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {},
   "outputs": [],
   "source": [
    "sumed = sumed.map_partitions(map_expand, label_map=label_map, meta=full_dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for item in symptom_vector:\n",
    "#     sumed[item] = sumed.SYMPTOM_CODE.apply(check, comp=label_map.get(item), meta=('SYMPTOM_CODE', np.uint8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/Users/teliov/TUD/Thesis/Medvice/Notebooks/data/exploration_II/output/parsed_df/dfdata-0.csv',\n",
       " '/Users/teliov/TUD/Thesis/Medvice/Notebooks/data/exploration_II/output/parsed_df/dfdata-1.csv']"
      ]
     },
     "execution_count": 407,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# and now we should have what we want ...\n",
    "# but we got into the one dataframe issue again. And it'll be tricky to verify the results because we also lost the condition index. \n",
    "# but let's see ..\n",
    "dffcsv_output = pathutils.get_data_file(\"exploration_II/output/parsed_df/dfdata-*.csv\")\n",
    "sumed.to_csv(dffcsv_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This was quite fast too!\n",
    "# but wwe also got the 1-partition business, so I don't know how this works on the cluster \n",
    "# but first let's see if this is correct!\n",
    "symp_pd = pd.read_csv(symptoms_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "metadata": {},
   "outputs": [],
   "source": [
    "dd = {item: np.uint8 for item in symptom_vector}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfsoln = pd.read_csv('/Users/teliov/TUD/Thesis/Medvice/Notebooks/data/exploration_II/output/parsed_df/dfdata-0.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "metadata": {},
   "outputs": [],
   "source": [
    "np_column_names = np.array(dfsoln.columns)[7:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_symptoms = symp_pd.groupby('ENCOUNTER')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "metadata": {},
   "outputs": [],
   "source": [
    "for val in dfsoln.itertuples():\n",
    "    encounterId = val[1]\n",
    "    recorded_symptoms = np_column_names[np.nonzero(val[8:])[0]]\n",
    "    encounter_symptoms = grouped_symptoms.get_group(encounterId).SYMPTOM_CODE\n",
    "    same_length = len(recorded_symptoms) == len(encounter_symptoms)\n",
    "    same_elem = set(recorded_symptoms) == set(encounter_symptoms)\n",
    "    assert same_length and same_elem, \"Expected both to be the same\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comments**\n",
    "\n",
    "This method does indeed work. \n",
    "\n",
    "It is a pity that I had to resort to using an apply function to get the bitwise and working as opposed to pandas most likely more efficient solution for handling these kind of broadcast operations.\n",
    "\n",
    "None the less, the question remains: will this fall within the memory requirements on the cluster ?\n",
    "\n",
    "For sure this method takes much less memory than the previous attempt. And I can do even more adjustments to make some more room, but we'll have to see if we can make something that does indeed work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
