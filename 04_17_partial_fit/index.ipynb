{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# /Users/teliov/TUD/Thesis/Medvice/Notebooks/data/04_06_new_data/data/split\n",
    "# So we can use the *thesislib* package\n",
    "import sys\n",
    "import os\n",
    "\n",
    "module_path = os.path.abspath(\"..\")\n",
    "\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from thesislib.utils import pathutils, knifeutils\n",
    "import json\n",
    "import math\n",
    "from collections import OrderedDict\n",
    "from glob import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "symptoms_csv = pathutils.get_data_file(\"04_06_new_data/data/partial/symptoms.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ratio = 0.8\n",
    "test_ratio = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first split into train and test set\n",
    "df = pd.read_csv(symptoms_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped = df.groupby('PATHOLOGY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "condition_sizes = grouped.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = pathutils.get_data_file(\"04_06_new_data/data/partial/data/\")\n",
    "for code, cnd_df in grouped.__iter__():\n",
    "    num_rows = cnd_df.shape[0]\n",
    "    train_count = int(math.ceil(train_ratio * num_rows))\n",
    "    train_df = cnd_df[:train_count]\n",
    "    test_df = cnd_df[train_count:]\n",
    "    train_file_name = output_path + \"train-%s.csv\" % code\n",
    "    test_file_name = output_path + \"test-%s.csv\" % code\n",
    "    \n",
    "    train_df.to_csv(train_file_name)\n",
    "    test_df.to_csv(test_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we have train and test splits that reflect the distribution of the conditions\n",
    "# next use cat to combined the split files together. <on the terminal> using:\n",
    "# cat file1 file2 ... filen > output\n",
    "# now we do the transformation on the train set to get it in the format we want"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "generating_synthea_modules = glob(pathutils.get_data_file(\"04_06_new_data/data/modules/*.json\"))\n",
    "num_unique_conditions = len(generating_synthea_modules)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "condition_symptom_map = {}\n",
    "condition_name_map = {}\n",
    "for module in generating_synthea_modules:\n",
    "    condition_code, condition_name, symptom_list = knifeutils.extract_condition_symptom_from_modules(module)\n",
    "    condition_symptom_map[condition_code] = symptom_list\n",
    "    condition_name_map[condition_code] = condition_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "symptom_vector = set()\n",
    "for condition_symptoms in condition_symptom_map.values():\n",
    "    symptom_vector = symptom_vector.union(condition_symptoms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "symptom_vector = sorted(symptom_vector)\n",
    "symptom_label_map = OrderedDict({})\n",
    "power = np.array([2**idx for idx in range(len(symptom_vector))])\n",
    "for idx, item in enumerate(symptom_vector):\n",
    "    symptom_label_map[item] = power[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "condition_codes = sorted(condition_name_map.keys())\n",
    "condition_label_map = OrderedDict()\n",
    "for idx, code in enumerate(condition_codes):\n",
    "    condition_label_map[code] = idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "untransformed_input = pathutils.get_data_file(\"04_06_new_data/data/partial/data/train.csv\")\n",
    "transformed_output = pathutils.get_data_file(\"04_06_new_data/output/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "knifeutils.parse_data(untransformed_input, condition_label_map, symptom_label_map, transformed_output, use_header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we have a train.csv file in the way we want, and we can then attempt using the random forest classifier with the warm start attribute\n",
    "# but first we must split the file into subsets to simulate what we'll be dealing with\n",
    "# for now we'll attempt no cross validation and simply train on the entire set\n",
    "split_files = glob(pathutils.get_data_file(\"04_06_new_data/output/split/x*\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_files = sorted(split_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_weights = {idx: 1 for idx in range(9)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_clf = RandomForestClassifier(n_estimators=140, criterion='gini', max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features='auto', max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, bootstrap=True, oob_score=False, n_jobs=2, random_state=None, verbose=0, warm_start=True, class_weight=class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we're going to attempt an incremental learning\n",
    "columns_interest = ['LABEL', 'GENDER', 'RACE', 'AGE'] + symptom_vector\n",
    "all_columns = ['Unnamed: 0'] + columns_interest\n",
    "for idx, file in enumerate(split_files):\n",
    "    if idx == 0:\n",
    "        df = pd.read_csv(file, usecols=columns_interest)\n",
    "        n_estimators = 0\n",
    "    else:\n",
    "        df = pd.read_csv(file, usecols=columns_interest, names=all_columns)\n",
    "        n_estimators = 140\n",
    "    \n",
    "    ylabels = df.LABEL.values\n",
    "    xdata = df.drop(columns=['LABEL']).values\n",
    "    \n",
    "    rf_clf.n_estimators += n_estimators\n",
    "    rf_clf.fit(xdata, ylabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we now have a forest that has been fitted on the entire data set.\n",
    "# we can now test.\n",
    "# first we do need to get our test set ready.\n",
    "# for convenience, we will not split the test set into parts and will process as is\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "untransformed_test = pathutils.get_data_file(\"04_06_new_data/data/partial/data/test.csv\")\n",
    "transformed_test = pathutils.get_data_file(\"04_06_new_data/output/\")\n",
    "knifeutils.parse_data(untransformed_test, condition_label_map, symptom_label_map, transformed_test, use_header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv(transformed_test+ \"test.csv\", usecols=columns_interest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_labels = test_df.LABEL.values.reshape(test_df.shape[0], -1)\n",
    "test_data = test_df.drop(columns=['LABEL']).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_score = rf_clf.score(test_data, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2803, 49)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2803, 1)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# going to try and concat the split symptom files and then split using StratifiedShuffleSplit\n",
    "input_files = pathutils.get_data_file(\"04_06_new_data/output/split-full/x*\")\n",
    "output_path = pathutils.get_data_file(\"04_06_new_data/output/concat\")\n",
    "train_split = 0.8\n",
    "knifeutils.concatenate_and_split(input_files, output_path, train_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
